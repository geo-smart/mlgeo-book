
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>3.8 Robust Training &#8212; ML Geo Curriculum</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/styles.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3.9 Ensemble learning" href="3.9_ensemble_learning.html" />
    <link rel="prev" title="3.7 Random Forests" href="3.7_randomForest_regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/GeoSMART_logo.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">ML Geo Curriculum</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../about_this_book/about_this_book.html">
                    Machine Learning in the Geosciences
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  About this Book
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://geo-smart.github.io/index.html">
   Geosmart website
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about_this_book/acknowledgements.html">
   Acknowlegments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../about_this_book/0_mlgeo_project.html">
   Primer
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 1 - Open Source Ecosystem with Python
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/readme.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.1_open_reproducible_science.html">
   1.1 Open Reproducible Science
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.2_jupyter_environment.html">
   1.3 Jupyter Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.3_python_environment.html">
   1.3 Python Ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.4_computational_environments.html">
   1.4 Computing Environments
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.5_version_control_git.html">
   1.5 Version Control &amp; GitHub
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.6_data_gallery.html">
   1.6 Data Gallery
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter1-GettingStarted/1.20_MLGEO_Final_Project.html">
   Final Integrated Project in Machine Learning in Geoscience
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 2 - Data Manipulation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.1_Data_Definitions.html">
   2.1 Data Definitions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.2_data_formats_rendered.html">
   2.2 Data Formats
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.3_pandas_rendered.html">
   2.3 Pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.4_dataframes_prep.html">
   2.4 DataFrame Exploration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.5_Arrays.html">
   2.5 Data Arrays
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.6_resampling.html">
   2.6 Resampling Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.7_statistical_considerations.html">
   2.7 Statistical Considerations for geoscientific Data and Noise
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.8_data_spectral_transforms.html">
   2.8 Spectral Transforms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.9_filtering_data.html">
   2.9 Filtering Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.10_synthetic_noise.html">
   2.10 Synthetic noise
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.11_feature_engineering.html">
   2.11 Feature Engineering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.12_dimensionality_reduction.html">
   2.12 Dimensionality Reduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.13_MLready_data.html">
   2.13 ML-ready data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter2-DataManipulation/2.20_Final_Project_Assignement.html">
   Assignment:
   <strong>
    Preparing AI-Ready Data for The Final Project
   </strong>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 3 - Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="3.1_concepts_supervision.html">
   3.1 Concepts in training supervision
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.2_classification_regression.html">
   3.2 Classification and Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.3_clustering.html">
   3.3 Clustering: Unsupervied Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.4_binary_classification.html">
   3.4 Binary classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.5_multiclass_classification.html">
   3.5 Multiclass Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.6_logistic_regression.html">
   3.6 Logistic regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.7_randomForest_regression.html">
   3.7 Random Forests
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3.8 Robust Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.9_ensemble_learning.html">
   3.9 Ensemble learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.10_autoML.html">
   3.10 AutoML
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="3.20_final_project_cml.html">
   <strong>
    Final Project - Classic Machine Learning
   </strong>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Homework_CML.html">
   Homework Classic Machine Learning (50 points)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 4 - Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.0_perceptrons.html">
   4.0 The Perceptron
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.1_NN.html">
   4.1 Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.2_MLP.html">
   4.2 Multi Layer Perceptrons
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.3_CNN.html">
   4.3 Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.4_RNN.html">
   4.4  Recurrent Neural Networks: Processing sequences
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.5_ModelTraining.html">
   4.5 Model Training
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.6_AutoEncoder.html">
   4.6 Auto-encoders
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.7_PINN.html">
   4.7 Physics-Informed Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.8_NAS.html">
   4.8 NAS: Network Architecture Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter4-DeepLearning/mlgeo_4.20_final_project_assignement.html">
   <strong>
    Deep Learning Exploration with AI-Ready Datasets
   </strong>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 5 - Workflow Management and Reproducibility
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Chapter5-ModelWorkflows/readme.html">
   This chapter focuces on model workflow and ML reproducibility
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Chapter 6- Introduction to Cloud Computing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/cloudmaven">
   Browser Access to Cloud Instances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://github.com/Denolle-Lab/azure">
   Terraform Access to Cloud Instances
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference external" href="https://tljh.jupyter.org/en/latest/">
   Cloud Provider ML Jupyterhubs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../reference/glossary.html">
   Glossaries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../reference/bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/geo-smart/mlgeo-book/main?urlpath=lab/tree/book/Chapter3-MachineLearning/3.8_robust_training.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/geo-smart/mlgeo-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/geo-smart/mlgeo-book/issues/new?title=Issue%20on%20page%20%2FChapter3-MachineLearning/3.8_robust_training.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/geo-smart/mlgeo-book/edit/main/book/Chapter3-MachineLearning/3.8_robust_training.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/Chapter3-MachineLearning/3.8_robust_training.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cross-validation">
   1. Cross validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#leave-one-out-cross-validation">
   2.5 Leave One Out Cross Validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-fold-cross-validation">
   2.6 K-fold cross validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyper-parameter-tuning">
   2. Hyper-parameter tuning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grid-search-cross-validation">
   1. Grid Search cross validation.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-search-cross-validation">
   2. Random Search Cross Validation.
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>3.8 Robust Training</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cross-validation">
   1. Cross validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#leave-one-out-cross-validation">
   2.5 Leave One Out Cross Validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-fold-cross-validation">
   2.6 K-fold cross validation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyper-parameter-tuning">
   2. Hyper-parameter tuning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#grid-search-cross-validation">
   1. Grid Search cross validation.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#random-search-cross-validation">
   2. Random Search Cross Validation.
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="robust-training">
<h1>3.8 Robust Training<a class="headerlink" href="#robust-training" title="Permalink to this headline">#</a></h1>
<p>This chapter will review two foundamental concepts and strategies for training machine learning models:</p>
<ol class="simple">
<li><p>Cross-Validation</p></li>
<li><p>Hyper-parameter tuning</p></li>
</ol>
<section id="cross-validation">
<h2>1. Cross validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">#</a></h2>
<p>Cross-validation is a widely used technique in machine learning to assess the <strong>performance</strong> and <strong>generalization</strong> of a model. It involves partitioning the dataset into multiple subsets, training the model on a portion of the data, and then evaluating its performance on the remaining data. This process is repeated several times, and <em>the results are averaged</em> to obtain a more reliable estimate of the model’s performance. The primary goal is <em>to mitigate the risk of overfitting and obtain a more robust evaluation metric</em>.</p>
<p>There are several tutorials on <a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">cross-validation</a>.</p>
<p>Cross-validation divides the data set between a <em>training</em> set and a <em>validation</em> set:</p>
<p><img alt="Validation Set Approach" src="../_images/ValsetApproach.png" />
From: scikit-learn: concept for training and validation set.</p>
<p>Often the validation set ends up underestimating the prediction errors (model uncertainties) because the validation set is often smaller than the training set. To alleviate that, we can perform cross-validation over many folds of selecting a validation and training set.</p>
<p><img alt="Validation Set Approach" src="../_images/grid_search_cross_validation.png" />
From: scikit-learn</p>
<ul class="simple">
<li><p><strong>Data Splitting</strong>: Cross-validation involves <em>partitioning a dataset into multiple subsets</em>, typically a training set and a testing set. This splitting allows for model assessment and validation.</p></li>
<li><p><strong>Estimation</strong>: Cross-validation is primarily used to <em>assess the predictive performance</em> of machine learning models or statistical models. It helps in estimating how well a model will generalize to new, unseen data by testing its performance on data that was not used in training.</p></li>
<li><p><strong>Correlated Data</strong>: Cross-validation does not explicitly address correlated data, and its effectiveness can be influenced by the data splitting strategy. <em>When data exhibits strong correlations, it’s essential to carefully design the cross-validation procedure to ensure that all subsets represent the overall data distribution</em>.</p></li>
<li><p><strong>Applications</strong>: Cross-validation is widely employed in <em>model selection</em>, <em>hyperparameter tuning</em>, and <em>assessing the generalization ability</em> of models.</p></li>
</ul>
<p>We will now randomly select a training and validation set using the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> module on a GNSS time series and using a linear regression problem</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span><span class="o">,</span> <span class="nn">zipfile</span><span class="o">,</span> <span class="nn">io</span><span class="o">,</span> <span class="nn">gzip</span><span class="o">,</span> <span class="nn">glob</span><span class="o">,</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The station designation</span>
<span class="n">sta</span><span class="o">=</span><span class="s2">&quot;P395&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;http://geodesy.unr.edu/gps_timeseries/tenv/IGS14/&quot;</span> <span class="o">+</span> <span class="n">sta</span> <span class="o">+</span> <span class="s2">&quot;.tenv&quot;</span><span class="p">)</span>
<span class="n">zip_file_url</span><span class="o">=</span><span class="s2">&quot;http://geodesy.unr.edu/gps_timeseries/tenv/IGS14/&quot;</span><span class="o">+</span> <span class="n">sta</span> <span class="o">+</span> <span class="s2">&quot;.tenv&quot;</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">zip_file_url</span><span class="p">)</span>


<span class="c1"># create a list of strings with itemized list above</span>
<span class="n">ll</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;station ID (SSSS)&#39;</span><span class="p">,</span><span class="s1">&#39;date (yymmmdd)&#39;</span><span class="p">,</span>
<span class="s1">&#39;decimal year&#39;</span><span class="p">,</span><span class="s1">&#39;modified Julian day&#39;</span><span class="p">,</span><span class="s1">&#39;GPS week&#39;</span><span class="p">,</span><span class="s1">&#39;day of GPS week&#39;</span><span class="p">,</span>
<span class="s1">&#39;longitude (degrees) of reference meridian&#39;</span><span class="p">,</span><span class="s1">&#39;delta e (m)&#39;</span><span class="p">,</span>
<span class="s1">&#39;delta n (m)&#39;</span><span class="p">,</span><span class="s1">&#39;delta v (m)&#39;</span><span class="p">,</span><span class="s1">&#39;antenna height (m)&#39;</span><span class="p">,</span>
<span class="s1">&#39;sigma e (m)&#39;</span><span class="p">,</span><span class="s1">&#39;sigma n (m)&#39;</span><span class="p">,</span><span class="s1">&#39;sigma v (m)&#39;</span><span class="p">,</span>
<span class="s1">&#39;correlation en&#39;</span><span class="p">,</span><span class="s1">&#39;correlation ev&#39;</span><span class="p">,</span><span class="s1">&#39;correlation nv&#39;</span><span class="p">]</span>
      

<span class="c1"># transform r.content into a pandas dataframe</span>
<span class="c1"># first split r.content with \n separator</span>
<span class="c1"># Decode the content if it&#39;s in bytes</span>
<span class="n">content_str</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s1">&#39;utf-8&#39;</span><span class="p">)</span>

<span class="c1"># Split the content by the newline character</span>
<span class="n">lines</span> <span class="o">=</span> <span class="n">content_str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Now `lines` is a list of strings, each representing a line from the content</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lines</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># then transform lines into a pandas dataframe</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">])</span>
<span class="c1"># assign column names to df a</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">ll</span>

<span class="c1">#convert columns to numeric</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>http://geodesy.unr.edu/gps_timeseries/tenv/IGS14/P395.tenv
P395 06JAN25 2006.0671 53760 1359 3 -123.9  3347.67917   4987420.31375   53.03678  0.0083 0.00069 0.00105 0.00327 -0.04832  0.01695 -0.31816
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>station ID (SSSS)</th>
      <th>date (yymmmdd)</th>
      <th>decimal year</th>
      <th>modified Julian day</th>
      <th>GPS week</th>
      <th>day of GPS week</th>
      <th>longitude (degrees) of reference meridian</th>
      <th>delta e (m)</th>
      <th>delta n (m)</th>
      <th>delta v (m)</th>
      <th>antenna height (m)</th>
      <th>sigma e (m)</th>
      <th>sigma n (m)</th>
      <th>sigma v (m)</th>
      <th>correlation en</th>
      <th>correlation ev</th>
      <th>correlation nv</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>P395</td>
      <td>06JAN25</td>
      <td>2006.0671</td>
      <td>53760.0</td>
      <td>1359.0</td>
      <td>3.0</td>
      <td>-123.9</td>
      <td>3347.67917</td>
      <td>4.987420e+06</td>
      <td>53.03678</td>
      <td>0.0083</td>
      <td>0.00069</td>
      <td>0.00105</td>
      <td>0.00327</td>
      <td>-0.04832</td>
      <td>0.01695</td>
      <td>-0.31816</td>
    </tr>
    <tr>
      <th>1</th>
      <td>P395</td>
      <td>06JAN26</td>
      <td>2006.0698</td>
      <td>53761.0</td>
      <td>1359.0</td>
      <td>4.0</td>
      <td>-123.9</td>
      <td>3347.68086</td>
      <td>4.987420e+06</td>
      <td>53.03003</td>
      <td>0.0083</td>
      <td>0.00069</td>
      <td>0.00104</td>
      <td>0.00321</td>
      <td>-0.04648</td>
      <td>0.00271</td>
      <td>-0.30970</td>
    </tr>
    <tr>
      <th>2</th>
      <td>P395</td>
      <td>06JAN27</td>
      <td>2006.0726</td>
      <td>53762.0</td>
      <td>1359.0</td>
      <td>5.0</td>
      <td>-123.9</td>
      <td>3347.68072</td>
      <td>4.987420e+06</td>
      <td>53.03906</td>
      <td>0.0083</td>
      <td>0.00069</td>
      <td>0.00105</td>
      <td>0.00326</td>
      <td>-0.02367</td>
      <td>0.00817</td>
      <td>-0.31941</td>
    </tr>
    <tr>
      <th>3</th>
      <td>P395</td>
      <td>06JAN28</td>
      <td>2006.0753</td>
      <td>53763.0</td>
      <td>1359.0</td>
      <td>6.0</td>
      <td>-123.9</td>
      <td>3347.67938</td>
      <td>4.987420e+06</td>
      <td>53.04382</td>
      <td>0.0083</td>
      <td>0.00069</td>
      <td>0.00105</td>
      <td>0.00324</td>
      <td>-0.03681</td>
      <td>0.00908</td>
      <td>-0.30515</td>
    </tr>
    <tr>
      <th>4</th>
      <td>P395</td>
      <td>06JAN29</td>
      <td>2006.0780</td>
      <td>53764.0</td>
      <td>1360.0</td>
      <td>0.0</td>
      <td>-123.9</td>
      <td>3347.68042</td>
      <td>4.987420e+06</td>
      <td>53.03513</td>
      <td>0.0083</td>
      <td>0.00068</td>
      <td>0.00105</td>
      <td>0.00328</td>
      <td>-0.04815</td>
      <td>0.00619</td>
      <td>-0.33029</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># remove first value for delta e, delta n, delta v to make relative position with respect to the first time. Add these as new columns</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;new delta e (m)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;delta e (m)&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;delta e (m)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;new delta n (m)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;delta n (m)&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;delta n (m)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;new delta v (m)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;delta v (m)&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;delta v (m)&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># drop nans in new delta e (m) and decimal year columns</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;new delta e (m)&#39;</span><span class="p">,</span> <span class="s1">&#39;decimal year&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="c1"># convert the data into numpy arrays.</span>
<span class="n">E</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;new delta e (m)&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="c1"># reshaping was necessary to be an argument of Linear regress</span>
<span class="c1"># E = np.asarray(df[&#39;east&#39;][df[&#39;station&#39;]==sta]).reshape(-1, 1)# reshaping was necessary to be an argument of Linear regress</span>
<span class="c1"># make a new time array</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;decimal year&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">tt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">t</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">t</span><span class="p">),</span><span class="mi">1000</span><span class="p">)</span>

<span class="c1"># perform the linear regression. First we will use the entire available data</span>
<span class="n">regr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="c1"># we will first perform the fit:</span>
<span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">E</span><span class="p">)</span>
<span class="c1"># We will first predict the fit:</span>
<span class="n">Epred</span><span class="o">=</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> 

<span class="c1"># The coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficient / Velocity eastward (mm/year): &#39;</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># plot the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">E</span><span class="p">,</span><span class="s1">&#39;b&#39;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Coefficient / Velocity eastward (mm/year):  -0.006439731291127403
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x12f725b50&gt;]
</pre></div>
</div>
<img alt="../_images/3.8_robust_training_5_2.png" src="../_images/3.8_robust_training_5_2.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># we randomly select values and split the data between training and validation set.</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span>
<span class="c1"># we split once the data between a training and a validating set </span>
<span class="n">n</span><span class="o">=</span><span class="mi">1</span> <span class="c1"># we do this selection once</span>
<span class="n">v_size</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># 30% of the data will be randomly selected to be the validation set.</span>

<span class="n">rs</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="n">rs</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">E</span><span class="p">):</span>
    <span class="n">E_train</span><span class="p">,</span> <span class="n">E_val</span> <span class="o">=</span> <span class="n">E</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">E</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
    <span class="n">t_train</span><span class="p">,</span> <span class="n">t_val</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">t</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">t_train</span><span class="p">,</span><span class="n">E_train</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;East (mm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">t_val</span><span class="p">,</span><span class="n">E_val</span><span class="p">,</span><span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span><span class="n">s</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span><span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Time (years)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;East component&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;training set&#39;</span><span class="p">,</span><span class="s1">&#39;validation set&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x30f3e3ee0&gt;
</pre></div>
</div>
<img alt="../_images/3.8_robust_training_6_1.png" src="../_images/3.8_robust_training_6_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="c1"># now fit the data on the training set.</span>
<span class="n">regr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="c1"># Fit on training data:</span>
<span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">t_train</span><span class="p">,</span><span class="n">E_train</span><span class="p">)</span>
<span class="c1"># We will first predict the fit:</span>
<span class="n">Epred</span><span class="o">=</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">t_train</span><span class="p">)</span> 
<span class="n">Epred_val</span><span class="o">=</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">t_val</span><span class="p">)</span> 

<span class="c1"># The coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Training set: Coefficient / Velocity eastward (mm/year): &#39;</span><span class="p">,</span> <span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MSE (mean square error) on training set (mm): </span><span class="si">%.2f</span><span class="s1">&#39;</span>
      <span class="o">%</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Epred</span><span class="p">,</span> <span class="n">E_train</span><span class="p">))</span>
<span class="c1"># The coefficient of determination: 1 is the perfect prediction</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Coefficient of determination on training set: </span><span class="si">%.2f</span><span class="s1">&#39;</span>
      <span class="o">%</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Epred</span><span class="p">,</span> <span class="n">E_train</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MSE on validation set (mm): </span><span class="si">%.2f</span><span class="s1"> and coefficient of determiniation on </span><span class="si">%.2f</span><span class="s1">&#39;</span> <span class="o">%</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">Epred_val</span><span class="p">,</span> <span class="n">E_val</span><span class="p">),</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">Epred_val</span><span class="p">,</span> <span class="n">E_val</span><span class="p">)))</span>


<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">E</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">);</span><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;East (mm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_train</span><span class="p">,</span><span class="n">Epred</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_val</span><span class="p">,</span><span class="n">Epred_val</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;data&#39;</span><span class="p">,</span><span class="s1">&#39;fit on training&#39;</span><span class="p">,</span><span class="s1">&#39;fit on validation&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Random selection for data split&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training set: Coefficient / Velocity eastward (mm/year):  -0.006437910455226973
MSE (mean square error) on training set (mm): 0.00
Coefficient of determination on training set: 0.99
MSE on validation set (mm): 0.00 and coefficient of determiniation on 0.99
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Random selection for data split&#39;)
</pre></div>
</div>
<img alt="../_images/3.8_robust_training_7_2.png" src="../_images/3.8_robust_training_7_2.png" />
</div>
</div>
</section>
<section id="leave-one-out-cross-validation">
<h2>2.5 Leave One Out Cross Validation<a class="headerlink" href="#leave-one-out-cross-validation" title="Permalink to this headline">#</a></h2>
<p>LOOCV splits the data in 2 sets (training and validation) <i>n</i> times (<i>n</i> is the number of data points). At each repeat, the training set is <strong>all but one</strong> data, the validation set is one element.</p>
<p><img alt="Validation Set Approach" src="../_images/LOOCV.png" />
Advantages: it has far less bias with respect to the training data. It does not overestimate the test error. Repeated LOOCV will give the exact same results.</p>
<p>Disadvantages: it is computationally intensive.</p>
<ul class="simple">
<li><p><strong>Data Splitting</strong>: LOOCV is an extreme form of cross-validation where, for each iteration, only one data point is left out as the test set, and the remaining data is used as the training set. This process is repeated for each data point, effectively creating as many folds as there are data points.</p></li>
<li><p><strong>Estimation</strong>: LOOCV is primarily used for assessing model performance and estimating predictive accuracy. By evaluating the model against all data points one at a time, LOOCV provides a robust assessment of a model’s ability to generalize to unseen data.</p></li>
<li><p><strong>Correlated Data</strong>: LOOCV, like other cross-validation methods, may not explicitly address correlated data. However, its performance can be influenced by the correlation structure in the data. For datasets with strong correlations, LOOCV may lead to models that are overly optimistic since it often tests a model on data points that are closely related to the training set.</p></li>
<li><p><strong>Applications</strong>: LOOCV is valuable for evaluating machine learning and statistical models, particularly when you have a limited amount of data.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">LeaveOneOut</span>
<span class="n">loo</span> <span class="o">=</span> <span class="n">LeaveOneOut</span><span class="p">()</span>

<span class="n">vel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">E</span><span class="p">))</span> <span class="c1"># initalize a vector to store the regression values</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">E</span><span class="p">))</span>
<span class="n">mse_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">E</span><span class="p">))</span>
<span class="n">r2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">E</span><span class="p">))</span>
<span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">loo</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">E</span><span class="p">):</span>    
    <span class="n">E_train</span><span class="p">,</span> <span class="n">E_val</span> <span class="o">=</span> <span class="n">E</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">E</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
    <span class="n">t_train</span><span class="p">,</span> <span class="n">t_val</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">t</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
    <span class="c1"># now fit the data on the training set.</span>
    <span class="n">regr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="c1"># Fit on training data:</span>
    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">t_train</span><span class="p">,</span><span class="n">E_train</span><span class="p">)</span>
    <span class="c1"># We will first predict the fit:</span>
    <span class="n">Epred_train</span><span class="o">=</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">t_train</span><span class="p">)</span> 
    <span class="n">Epred_val</span><span class="o">=</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">t_val</span><span class="p">)</span> 

    <span class="c1"># The coefficients</span>
    <span class="n">vel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mse_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">E_train</span><span class="p">,</span> <span class="n">Epred_train</span><span class="p">)</span>
    <span class="n">mse_val</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">E_val</span><span class="p">,</span> <span class="n">Epred_val</span><span class="p">)</span>
    <span class="n">r2s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">r2_score</span><span class="p">(</span><span class="n">E_val</span><span class="p">,</span> <span class="n">Epred_val</span><span class="p">)</span>
    <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>

<span class="c1"># the data shows clearly a trend, so the predictions of the trends are close to each other:</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mean of the velocity estimates </span><span class="si">%f</span><span class="s2">4.2 and the standard deviation </span><span class="si">%f</span><span class="s2">4.2&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">vel</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">vel</span><span class="p">)))</span>
<span class="c1"># the test error is the average of the mean-square-errors</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CV = </span><span class="si">%4.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_val</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean of the velocity estimates -0.0064404.2 and the standard deviation 0.0000004.2
CV = 0.00
</pre></div>
</div>
</div>
</div>
<p>LOOCV is rarely used in practice. This example is just to show the extreme end member of cross-fold validation.</p>
</section>
<section id="k-fold-cross-validation">
<h2>2.6 K-fold cross validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this headline">#</a></h2>
<p>Designed to reduce the computational cost of LOOCV. Randomly devide over <i>k</i> groups/folds of approximately equal size. It is typical to use <i>5</i> or <i>10</i>.</p>
<p><img alt="Validation Set Approach" src="../_images/Kfold.png" /></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="c1"># let&#39;s try on 10-folds, 10 adjacent split of the data.</span>
<span class="n">k</span><span class="o">=</span><span class="mi">10</span>
<span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>

<span class="n">vel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="c1"># initalize a vector to store the regression values</span>
<span class="n">mse_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">mse_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">r2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
<span class="n">i</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">val_index</span> <span class="ow">in</span> <span class="n">kf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">E</span><span class="p">):</span>    
    <span class="n">E_train</span><span class="p">,</span> <span class="n">E_val</span> <span class="o">=</span> <span class="n">E</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">E</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
    <span class="n">t_train</span><span class="p">,</span> <span class="n">t_val</span> <span class="o">=</span> <span class="n">t</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">t</span><span class="p">[</span><span class="n">val_index</span><span class="p">]</span>
    <span class="c1"># now fit the data on the training set.</span>
    <span class="n">regr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    <span class="c1"># Fit on training data:</span>
    <span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">t_train</span><span class="p">,</span><span class="n">E_train</span><span class="p">)</span>
    <span class="c1"># We will first predict the fit:</span>
    <span class="n">Epred_train</span><span class="o">=</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">t_train</span><span class="p">)</span> 
    <span class="n">Epred_val</span><span class="o">=</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">t_val</span><span class="p">)</span> 

    <span class="c1"># The coefficients</span>
    <span class="n">vel</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">mse_val</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">E_val</span><span class="p">,</span> <span class="n">Epred_val</span><span class="p">)</span>
    <span class="n">mse_train</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">E_train</span><span class="p">,</span> <span class="n">Epred_train</span><span class="p">)</span>
    <span class="n">r2s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="n">r2_score</span><span class="p">(</span><span class="n">E_train</span><span class="p">,</span> <span class="n">Epred_train</span><span class="p">)</span>
    <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>

<span class="c1"># the data shows clearly a trend, so the predictions of the trends are close to each other:</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mean of the velocity estimates </span><span class="si">%4.2f</span><span class="s2"> and the standard deviation </span><span class="si">%4.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">vel</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">vel</span><span class="p">)))</span>
<span class="c1"># the test error is the average of the mean-square-errors</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mean MSE for training set : </span><span class="si">%4.2f</span><span class="s2"> and the validation set: </span><span class="si">%4.2f</span><span class="s2">&quot;</span><span class="o">%</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_train</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">mse_val</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean of the velocity estimates -0.01 and the standard deviation 0.00
mean MSE for training set : 0.00 and the validation set: 0.00
</pre></div>
</div>
</div>
</div>
</section>
<section id="hyper-parameter-tuning">
<h2>2. Hyper-parameter tuning<a class="headerlink" href="#hyper-parameter-tuning" title="Permalink to this headline">#</a></h2>
<p>In classic machine learning, models often have parameters that are learned from the training data (such as weights in a linear regression model), and hyperparameters that are external configuration settings. Hyperparameters are not learned from the data but are set prior to the training process and can significantly impact the model’s performance.</p>
<p>The goal of hyperparameter tuning is to find the optimal combination of hyperparameter values that maximizes the model’s performance on a given dataset. This process helps fine-tune the model to achieve the best possible results and avoid overfitting or underfitting.</p>
<p>Hyper-parameter tuning is now <em><strong>standard</strong></em> and should be performed in every work.</p>
<p>Several approaches exists to hyper-parameter tuning:</p>
<ul>
<li><p><strong>Manual Tuning:</strong></p>
<ul class="simple">
<li><p>While more time-consuming, manual tuning involves domain experts iteratively adjusting hyperparameters based on their understanding of the problem and the model’s behavior.</p></li>
<li><p>This approach can be effective when the hyperparameter space is relatively small or when there is substantial domain knowledge.</p></li>
<li><p>This is a good step for initial exploration and intuition-building experiment, but systematic approaches like described below will be required for robust assessment.</p></li>
</ul>
</li>
<li><p><strong>Grid Search:</strong></p>
<ul class="simple">
<li><p>In this method, a predefined set of hyperparameter values is specified, and the model is trained and evaluated for all possible combinations.</p></li>
<li><p>While thorough, grid search can be computationally expensive, especially for a large number of hyperparameters or when the search space is extensive.</p></li>
<li><p>This method is implemented in the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> ecosystem as <code class="docutils literal notranslate"><span class="pre">model_selection.GridSearchCV</span></code>, which uses cross-validation to train-test each model given the tested hyper-parameters.</p></li>
</ul>
</li>
<li><p><strong>Random Search:</strong></p>
<ul class="simple">
<li><p>Random search involves randomly selecting combinations of hyperparameter values from a predefined distribution of hyperparameters (e.g., uniform or normal).</p></li>
<li><p>This approach is more computationally efficient than grid search, as it explores a diverse set of hyperparameter combinations.</p></li>
<li><p>The method is implement in the <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> ecosystem as <code class="docutils literal notranslate"><span class="pre">model_selection.RandomizedSearchCV</span></code>, which uses cross-validation for each</p></li>
</ul>
</li>
<li><p><strong>Bayesian Optimization:</strong></p>
<ul class="simple">
<li><p>Bayesian optimization employs probabilistic models to predict the performance of different hyperparameter configurations.</p></li>
<li><p>It adapts its search based on the results of previous evaluations, allowing it to focus on promising regions of the hyperparameter space.</p></li>
</ul>
<p>Below is a tutorial using the <code class="docutils literal notranslate"><span class="pre">digits</span></code> data sets.</p>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># basic tools</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataseta</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
<span class="n">digits</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;data&#39;, &#39;target&#39;, &#39;frame&#39;, &#39;feature_names&#39;, &#39;target_names&#39;, &#39;images&#39;, &#39;DESCR&#39;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># explore data type</span>
<span class="n">data</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span><span class="n">digits</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">][:]),</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="c1"># note that we do not modify the raw data that is stored on the digits dictionary.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;numpy.ndarray&#39;&gt; &lt;class &#39;numpy.int64&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>Plot the data</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">digits</span><span class="p">[</span><span class="s2">&quot;images&quot;</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.image.AxesImage at 0x30f759df0&gt;
</pre></div>
</div>
<img alt="../_images/3.8_robust_training_17_1.png" src="../_images/3.8_robust_training_17_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="nb">max</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="c1"># fit the model for data normalization</span>
<span class="n">newdata</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="c1"># transform the data. watch that data was converted to a numpy array</span>

<span class="c1"># Split data into 80% train and 20% test subsets</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;There are </span><span class="si">{</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> data samples&quot;</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.0 15.0
There are 1797 data samples
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>

<span class="c1"># Support Vector Machine classifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># learn</span>
<span class="n">knn_prediction</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span> <span class="c1"># predict on test</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SVC Accuracy:&quot;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span> <span class="p">,</span><span class="n">y_pred</span><span class="o">=</span><span class="n">knn_prediction</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>SVC Accuracy: 0.9666666666666667
</pre></div>
</div>
</div>
</div>
<p>what are the parameters we are trying to optimize?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;algorithm&#39;: &#39;auto&#39;,
 &#39;leaf_size&#39;: 30,
 &#39;metric&#39;: &#39;minkowski&#39;,
 &#39;metric_params&#39;: None,
 &#39;n_jobs&#39;: None,
 &#39;n_neighbors&#39;: 3,
 &#39;p&#39;: 2,
 &#39;weights&#39;: &#39;uniform&#39;}
</pre></div>
</div>
</div>
</div>
<p>A search consists of:</p>
<ul class="simple">
<li><p>an estimator (regressor or classifier such as <code class="docutils literal notranslate"><span class="pre">KNN()</span></code>);</p></li>
<li><p>a parameter space;</p></li>
<li><p>a method for searching or sampling candidates (grid search or random selection);</p></li>
<li><p>a cross-validation scheme; and</p></li>
<li><p>a loss function or a scoring metrics.</p></li>
</ul>
</section>
<section id="grid-search-cross-validation">
<h2>1. Grid Search cross validation.<a class="headerlink" href="#grid-search-cross-validation" title="Permalink to this headline">#</a></h2>
<p>Performs the search in the brute-force way using cross-validation. One has to define the parameter space. The scikit-learn function is <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>. More details <span class="xref myst">here</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span> <span class="s1">&#39;weights&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span><span class="s1">&#39;distance&#39;</span><span class="p">],</span> <span class="s1">&#39;algorithm&#39;</span><span class="p">:</span> <span class="p">[</span> <span class="s1">&#39;ball_tree&#39;</span><span class="p">,</span> <span class="s1">&#39;kd_tree&#39;</span><span class="p">],</span>
   <span class="s1">&#39;metric&#39;</span><span class="p">:[</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span><span class="s1">&#39;manhattan&#39;</span><span class="p">,</span><span class="s1">&#39;chebyshev&#39;</span><span class="p">,</span><span class="s1">&#39;minkowski&#39;</span><span class="p">]}</span>
 <span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The algorithm will search for all combinations of parameters, which can be from the model algorithms or the choice of features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># learn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Fitting 5 folds for each of 160 candidates, totalling 800 fits
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.972 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.941 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.986 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.986 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.906 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.941 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.972 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.913 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.941 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.969 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.986 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.941 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.917 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.913 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.913 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.937 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.913 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.937 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.937 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.937 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.913 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.955 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.951 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.958 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.962 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.937 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.951 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.958 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.955 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.951 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.958 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.944 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.955 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.917 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.944 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.944 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.920 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.951 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.923 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.951 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.927 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.955 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.958 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.972 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.941 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.955 total time=   0.1s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=ball_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=uniform;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=1, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=2, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.972 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=3, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.941 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=4, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=5, weights=distance;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=7, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=8, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=9, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=euclidean, n_neighbors=10, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.986 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.986 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.955 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.941 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=2, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.972 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=3, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.913 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.941 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=uniform;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.969 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=4, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.986 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=5, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.941 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.969 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=7, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.910 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.930 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=uniform;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.917 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=8, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.917 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=uniform;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.917 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.934 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=9, weights=distance;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.906 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.937 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.962 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.913 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.937 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.969 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=manhattan, n_neighbors=10, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.941 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.941 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=uniform;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.941 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.941 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=1, weights=distance;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.941 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.934 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=uniform;, score=0.951 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.934 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=2, weights=distance;, score=0.955 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.944 total time=   0.1s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.937 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=uniform;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.951 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=3, weights=distance;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=uniform;, score=0.958 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=4, weights=distance;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.948 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=uniform;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.958 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.958 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=5, weights=distance;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.955 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.941 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=uniform;, score=0.962 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.955 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=6, weights=distance;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.951 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.937 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.941 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=uniform;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.958 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.948 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.927 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=uniform;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.955 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.937 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=8, weights=distance;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.951 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.920 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=uniform;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.962 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.923 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=9, weights=distance;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.941 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.951 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.913 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=uniform;, score=0.965 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.955 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.923 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=chebyshev, n_neighbors=10, weights=distance;, score=0.969 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=uniform;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=1, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.948 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.983 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.965 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=2, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.972 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=uniform;, score=0.990 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.948 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=3, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=uniform;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.941 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.990 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.976 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=4, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=5, weights=distance;, score=0.976 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.938 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.965 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.948 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.944 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.965 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=6, weights=distance;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.934 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.951 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=7, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.920 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.976 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.955 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=8, weights=distance;, score=0.986 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.944 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=uniform;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.931 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.972 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.951 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=9, weights=distance;, score=0.983 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.924 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.969 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=uniform;, score=0.979 total time=   0.0s
[CV 1/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.927 total time=   0.0s
[CV 2/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.969 total time=   0.0s
[CV 3/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.944 total time=   0.0s
[CV 4/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.958 total time=   0.0s
[CV 5/5] END algorithm=kd_tree, metric=minkowski, n_neighbors=10, weights=distance;, score=0.983 total time=   0.0s
</pre></div>
</div>
<div class="output text_html"><style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(n_neighbors=3),
             param_grid=[{&#x27;algorithm&#x27;: [&#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;],
                          &#x27;metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;, &#x27;chebyshev&#x27;,
                                     &#x27;minkowski&#x27;],
                          &#x27;n_neighbors&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                          &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]}],
             verbose=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox" ><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(cv=5, estimator=KNeighborsClassifier(n_neighbors=3),
             param_grid=[{&#x27;algorithm&#x27;: [&#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;],
                          &#x27;metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;, &#x27;chebyshev&#x27;,
                                     &#x27;minkowski&#x27;],
                          &#x27;n_neighbors&#x27;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
                          &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]}],
             verbose=3)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox" ><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-9" type="checkbox" ><label for="sk-estimator-id-9" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">search</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;cv&#39;: 5,
 &#39;error_score&#39;: nan,
 &#39;estimator__algorithm&#39;: &#39;auto&#39;,
 &#39;estimator__leaf_size&#39;: 30,
 &#39;estimator__metric&#39;: &#39;minkowski&#39;,
 &#39;estimator__metric_params&#39;: None,
 &#39;estimator__n_jobs&#39;: None,
 &#39;estimator__n_neighbors&#39;: 3,
 &#39;estimator__p&#39;: 2,
 &#39;estimator__weights&#39;: &#39;uniform&#39;,
 &#39;estimator&#39;: KNeighborsClassifier(n_neighbors=3),
 &#39;n_jobs&#39;: None,
 &#39;param_grid&#39;: [{&#39;n_neighbors&#39;: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
   &#39;weights&#39;: [&#39;uniform&#39;, &#39;distance&#39;],
   &#39;algorithm&#39;: [&#39;ball_tree&#39;, &#39;kd_tree&#39;],
   &#39;metric&#39;: [&#39;euclidean&#39;, &#39;manhattan&#39;, &#39;chebyshev&#39;, &#39;minkowski&#39;]}],
 &#39;pre_dispatch&#39;: &#39;2*n_jobs&#39;,
 &#39;refit&#39;: True,
 &#39;return_train_score&#39;: False,
 &#39;scoring&#39;: None,
 &#39;verbose&#39;: 3}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">search</span><span class="o">.</span><span class="n">cv_results_</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;mean_fit_time&#39;, &#39;std_fit_time&#39;, &#39;mean_score_time&#39;, &#39;std_score_time&#39;, &#39;param_algorithm&#39;, &#39;param_metric&#39;, &#39;param_n_neighbors&#39;, &#39;param_weights&#39;, &#39;params&#39;, &#39;split0_test_score&#39;, &#39;split1_test_score&#39;, &#39;split2_test_score&#39;, &#39;split3_test_score&#39;, &#39;split4_test_score&#39;, &#39;mean_test_score&#39;, &#39;std_test_score&#39;, &#39;rank_test_score&#39;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">search</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;algorithm&#39;: &#39;ball_tree&#39;,
 &#39;metric&#39;: &#39;euclidean&#39;,
 &#39;n_neighbors&#39;: 1,
 &#39;weights&#39;: &#39;uniform&#39;}
</pre></div>
</div>
</div>
</div>
</section>
<section id="random-search-cross-validation">
<h2>2. Random Search Cross Validation.<a class="headerlink" href="#random-search-cross-validation" title="Permalink to this headline">#</a></h2>
<p>It performs the search in the brute-force way using cross-validation. One has to define the parameter space. The scikit-learn function is <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code>. More details <span class="xref myst">here</span>.</p>
<p>The advantage is that it can be used for a wide hyperparameter space and limit to <code class="docutils literal notranslate"><span class="pre">n_iter</span></code> number of iterations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span>  <span class="n">randint</span>


<span class="n">distributions</span><span class="o">=</span> <span class="p">[</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="n">randint</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">high</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span> <span class="s1">&#39;weights&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span><span class="s1">&#39;distance&#39;</span><span class="p">],</span> <span class="s1">&#39;algorithm&#39;</span><span class="p">:</span> <span class="p">[</span> <span class="s1">&#39;ball_tree&#39;</span><span class="p">,</span> <span class="s1">&#39;kd_tree&#39;</span><span class="p">],</span>
                  <span class="s1">&#39;metric&#39;</span><span class="p">:[</span><span class="s1">&#39;euclidean&#39;</span><span class="p">,</span><span class="s1">&#39;manhattan&#39;</span><span class="p">,</span><span class="s1">&#39;chebyshev&#39;</span><span class="p">,</span><span class="s1">&#39;minkowski&#39;</span><span class="p">]}</span> <span class="p">]</span>
<span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">distributions</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">clf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-4" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>RandomizedSearchCV(cv=5, estimator=KNeighborsClassifier(n_neighbors=3),
                   n_iter=100,
                   param_distributions=[{&#x27;algorithm&#x27;: [&#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;],
                                         &#x27;metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;,
                                                    &#x27;chebyshev&#x27;, &#x27;minkowski&#x27;],
                                         &#x27;n_neighbors&#x27;: array([9, 3, 5, 7, 8, 5, 6, 7, 7, 8]),
                                         &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]}],
                   random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-10" type="checkbox" ><label for="sk-estimator-id-10" class="sk-toggleable__label sk-toggleable__label-arrow">RandomizedSearchCV</label><div class="sk-toggleable__content"><pre>RandomizedSearchCV(cv=5, estimator=KNeighborsClassifier(n_neighbors=3),
                   n_iter=100,
                   param_distributions=[{&#x27;algorithm&#x27;: [&#x27;ball_tree&#x27;, &#x27;kd_tree&#x27;],
                                         &#x27;metric&#x27;: [&#x27;euclidean&#x27;, &#x27;manhattan&#x27;,
                                                    &#x27;chebyshev&#x27;, &#x27;minkowski&#x27;],
                                         &#x27;n_neighbors&#x27;: array([9, 3, 5, 7, 8, 5, 6, 7, 7, 8]),
                                         &#x27;weights&#x27;: [&#x27;uniform&#x27;, &#x27;distance&#x27;]}],
                   random_state=0)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-11" type="checkbox" ><label for="sk-estimator-id-11" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-12" type="checkbox" ><label for="sk-estimator-id-12" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=3)</pre></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">clf2</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;algorithm&#39;: &#39;ball_tree&#39;, &#39;metric&#39;: &#39;euclidean&#39;, &#39;n_neighbors&#39;: 1, &#39;weights&#39;: &#39;uniform&#39;}
{&#39;weights&#39;: &#39;distance&#39;, &#39;n_neighbors&#39;: 3, &#39;metric&#39;: &#39;minkowski&#39;, &#39;algorithm&#39;: &#39;kd_tree&#39;}
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Chapter3-MachineLearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="3.7_randomForest_regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">3.7 Random Forests</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="3.9_ensemble_learning.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3.9 Ensemble learning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By eScience Institute, University of Washington<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>