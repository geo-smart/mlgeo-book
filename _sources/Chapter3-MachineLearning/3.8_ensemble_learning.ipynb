{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8 Ensemble learning\n",
    "\n",
    "The group of models performs better than individual models. For example, Random Forests perform better than the individual Decision Trees that constitute the Forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits,fetch_openml\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# explore data type\n",
    "data,y = digits[\"data\"].copy(),digits[\"target\"].copy()\n",
    "print(type(data[0][:]),type(y[0]))\n",
    "# note that we do not modify the raw data that is stored on the digits dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1797 data samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(data)# fit the model for data normalization\n",
    "newdata = scaler.transform(data) # transform the data. watch that data was converted to a numpy array\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "print(f\"There are {data.shape[0]} data samples\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Voting Classifier\n",
    "\n",
    "Aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "![Voting Classifer](votingclassifier.png)\n",
    "\n",
    "From \"Hands on Machine Learning With Sci-kit Learn, Keras, and Tensorflow\" (Gueron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "svc_clf = SVC() # model design\n",
    "nb_clf = GaussianNB()\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [ ('nb',nb_clf) , ('rf',rf_clf),('svc',svc_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train) # learn\n",
    "y_pred=voting_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with individual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB 0.8263632119514472\n",
      "RandomForestClassifier 0.9621008403361344\n",
      "SVC 0.9793790849673202\n",
      "VotingClassifier 0.9632119514472456\n"
     ]
    }
   ],
   "source": [
    "for clf in (nb_clf,rf_clf,svc_clf,voting_clf):\n",
    "    scores = cross_val_score(clf, data, y, scoring='accuracy', cv=15)\n",
    "    print(clf.__class__.__name__,scores.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that taking the average of the predicted probability, instead of its max, is also a possibility. One can only evaluate the classifiers that output probabilities, which SVM does not do by default. Set the ``voting`` to ``soft`` to compare with the mean proba. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bagging and Pasting\n",
    "\n",
    "This approaches uses the same model algorithm but resampling on the training set. For resampling **with replacement** (bootstrap), it is called *bagging*; for resampling **without replacement**, it is called *pasting*.\n",
    "\n",
    "Several models are trained on different data, then the predictions are aggregated (*statistical mode* for classification and *average* for regression). The aggregated model tends to have a lower variance and bias, similar to what it would get if it were trained on the entire data.\n",
    "\n",
    "In the example below, you can implement BaggingClassifier in sklearn and explore by changing the number of model or bootstrapping value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.9610569483132156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator = KNeighborsClassifier(),n_estimators=100,  # n_estimator is the number of models to train\n",
    "    max_samples=1000,bootstrap=True, # bootstrap is for bagging vs pasting\n",
    "    n_jobs=-1,#number of CPU cores independently used for training and prediction. Use -1 for all available score\n",
    "    )\n",
    "scores=cross_val_score(bag_clf,data,y,cv=5)\n",
    "print('mean accuracy',scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One property of the resampling replacement is that for $m$ sampled data with replacement, up to 63% of the data tends to be sampled in average, as $m$ tends to the size of the total number of samples. This means that in average, 37% of the data never gets sampled. That portion act as a testing set. To get the accuracy score on that portion of the data, we can set the argument for BaggingClassifer ``oob_score=True`` and compare the with the score from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator = KNeighborsClassifier(),n_estimators=100,  # n_estimator is the number of models to train\n",
    "    max_samples=100,bootstrap=True, # bootstrap is for bagging vs pasting\n",
    "    n_jobs=-1,oob_score=True,#number of CPU cores independently used for training and prediction. Use -1 for all available score\n",
    "    )\n",
    "scores=cross_val_predict(bag_clf,X_test,y_test,cv=5) # the size of max_samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting\n",
    "\n",
    "The idea behind boosting methods is to train predictors sequentially, each trying to correct its predecessor.\n",
    "\n",
    "### 3.1 AdaBoost\n",
    "The AdaBoost algorithm trains a new predictor by paying more attention (up-weighting) the bad predictions from the previous predictor. For instance, in a classification, a first predictor will be underfitting the data and misclassifying labels. The second predictor weights more strongly the data that was misclassified. The ``learning_rate`` parameters set the magnitude of that weighting.\n",
    "\n",
    "AdaBoost work on any classifier that outputs probabilities (e.g., DecisionTrees, KNN; look for the classifiers that can take the function ``predict_proba()``).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7944444444444444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=100),n_estimators=2000,\n",
    "    algorithm='SAMME.R',learning_rate=0.01) # use the algorithm SAMME for binary classificaiton, SAMME.R for multi-class\n",
    "ada_clf.fit(X_train,y_train)\n",
    "y_pred=ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gradient Boosting\n",
    "\n",
    "Algorithm that sequentially fits the data and its predicted residuals:\n",
    "- Single small estimator is trained on the data (a *weak learner*)\n",
    "- Second small estimator is trained on the residuals between the data and the predicted data from the first estimator (a second *weak learner*). The residuals are getting smaller\n",
    "- A third small estimator...\n",
    "\n",
    "- The final prediction is the sum of all predictions.\n",
    "\n",
    "Gradient boosting is typically used using Decision Trees. ``n_estimators`` limits the total number of trees. When ``n_estimators`` is very large, the model will overfit the data. To regularize the training, one can find the optimal number of estimators by looking at the progressive reduction of the residuals until the residual curve flattens.\n",
    "\n",
    "The ``learning_rate`` is a hyperparameter that scales the contribution of each tree. When ``learn_rate`` is low, the GB will need more trees to git.\n",
    "\n",
    "The most popular algorithm is **XGBoost** for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.7.1-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/marinedenolle/.conda/envs/mlgeo_sk/lib/python3.9/site-packages (from xgboost) (1.23.4)\n",
      "Requirement already satisfied: scipy in /Users/marinedenolle/.conda/envs/mlgeo_sk/lib/python3.9/site-packages (from xgboost) (1.9.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "Nclass = len(np.unique(y_train))\n",
    "train = xgb.DMatrix(X_train,label=y_train)\n",
    "test = xgb.DMatrix(X_test,label=y_test)\n",
    "print(Nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth':40, # depth of the trees\n",
    "    'eta':0.3,  # learning rate\n",
    "    'objective':'multi:softmax',\n",
    "    'num_class':Nclass\n",
    "}\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgb.train(param,train,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8833333333333333"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stacking\n",
    "\n",
    "Each estimator may predict a value. Instead of taking the majority vote, one can aggregate the probabilities using another estimator. The last estimator is called a **meta learning** that takes all of the predictions and fit to the labeled prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mlgeo_sk')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6825af05e3ea1f79f5651bdd3095330bdaee3a1e3958825bcb0ebbb42c21bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
