{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 Logistic regression\n",
    "\n",
    "_Warning!_ Although it is called logistic _regression_, logistic regression is actually a _classification_ method.\n",
    "\n",
    "By Ariane Ducellier (2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we are going to talk about:\n",
    "- A simple classification method: Logistic regression\n",
    "- Gradient descent method\n",
    "- Automatic differentiation\n",
    "- Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember linear regression:\n",
    "\n",
    "$y = b + x w + \\epsilon$\n",
    "\n",
    "$y$ is a vector of length $n$, $x$ is a matrix with $n$ rows and $p$ columns, corresponding to $n$ observations and $p$ features that are used to explain $y$. \n",
    "\n",
    "$b$ is a scalar. $w$ is a vector of length $p$. $\\epsilon$ is a random error vector, of length $n$. It is independent of $x$, and has mean of zero.\n",
    "\n",
    "Our objective is to find the best values of $b$ and $w$ so that the values of $\\hat{y} = b + x w$ are as close as possible to the actual values $y$.\n",
    "\n",
    "For linear regression, $y$ is a quantitative variable. What if $y$ is a qualitative variable, for example $y = 0$ for \"no\", and $y = 1$ for \"yes\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to use regression to solve a classification problem is to model the probability of the variable $y$ taking the value 1:\n",
    "\n",
    "$P (y = 1) = b + x w$\n",
    "\n",
    "Once we have found the best values $\\hat{b}$ and $\\hat{w}$, we compute $\\hat{y} = \\hat{b} + x \\hat{w}$. If $\\hat{y} \\geq 0.5$, we decide to classify this observation as $y = 1$, that is \"yes\". If $\\hat{y} < 0.5$, we decide to classify this observation as $y = 0$, that is \"no\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with this method. We would like to have $0 \\leq P (y = 1) \\leq 1$ because it is a probability. However, there is nothing in this formulation that forces $b$ and $w$ to take values such that $\\hat{y} = \\hat{b} + x \\hat{w}$ will always take values in $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem, we can instead write:\n",
    "\n",
    "$z = b + x w$ and $P (y = 1) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "That way, we always have $0 \\leq P (y = 1) \\leq 1$. When $b + x w$ gets large, $P (y = 1)$ gets close to 1, and the value \"yes\" is more and more likely. When $b + x w$ gets small, $P (y = 1)$ gets close to 0, and the value \"no\" is more and more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the optimal value of $b$ and $w$? We define the cross-entropy loss function. For one observation, the loss function is:\n",
    "\n",
    "$\\mathcal{L} = - (y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i)$ with $\\hat{y}_i = \\frac{1}{1 + e^{- (b + x_i^T w)}}$ where $x_i$ is the $i$th row of x.\n",
    "\n",
    "If the true observation $y_i$ is 1 (\"yes\") and $\\hat{y}_i = 1$, the loss function takes the value 0. If $\\hat{y}_i = 0$, the loss function tends to infinity.\n",
    "\n",
    "If the true observation $y$ is 0 (\"no\") and $\\hat{y}_i = 0$, the loss function takes the value 0. If $\\hat{y}_i = 1$, the loss function tends to infinity.\n",
    "\n",
    "For all the $n$ observations, we write:\n",
    "\n",
    "$\\mathcal{L} = \\sum_{i = 1}^n \\mathcal{L}_i$\n",
    "\n",
    "Our objective is thus to find the values of $b$ and $\\omega$ that minimize the loss function. Note that with this formulation, $\\mathcal{L}$ is always positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "We know that the gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_j}$ is positive if the loss $\\mathcal{L}$ increases when $w_j$ increases. Reversely, the gradient $\\frac{\\partial \\mathcal{L}}{\\partial w_j}$ is negative if the loss $\\mathcal{L}$ decreases when $w_j$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain smaller and smaller values of the loss, at each iteration we take:\n",
    "\n",
    "$w_j^{(k + 1)} = w_j^{(k)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_j}$ for $j = 1 , \\cdots , p$\n",
    "\n",
    "$b^{(k + 1)} = b^{(k)} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
    "\n",
    "We assume that the value of $\\alpha$ is not too big. If the gradient is positive, then the value of $w_j$ will decrease at each iteration, and the value of the loss function will decrease. If the gradient is negative, then the value of $w_j$ will increase at each iteration, and the value of the loss will decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, all we need to do is to compute the gradient of the loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three ways of computing the gradient. The first method is to use the formula of the loss:\n",
    "\n",
    "$\\mathcal{L} (w_j , b) = - \\sum_{i = 1}^n y_i \\log (\\frac{1}{1 + \\exp (- b - \\sum_{j = 1}^p w_j x_{i,j})}) + (1 - y_i) \\log (1 - \\frac{1}{1 + \\exp (- b - \\sum_{j = 1}^p w_j x_{i,j})})$\n",
    "\n",
    "and to calculate the exact formula of the derivatives $\\frac{\\partial \\mathcal{L}}{\\partial w_j}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b}$. You just then have to implement the exact formula in the code to compute the gradient.\n",
    "\n",
    "When the formula gets more and more complicated, you become more and more likely to make a mistake, either in the calculation of the derivative formula, either in the implementation in your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method is to compute an approximation of the gradient:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w_j} = \\frac{\\mathcal{L}(w_j + \\Delta w_j) - \\mathcal{L}(w_j)}{\\Delta W_j}$\n",
    "\n",
    "If you write too many approximations, the method may not work very well and give inexact results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third method is to use automatic differentiation. If we write:\n",
    "\n",
    "$z = x_i^T w + b = f_x(w, b)$, $\\sigma = \\frac{1}{1 + e^{-z}} = g(z)$ and $L = - (y_i \\log(\\sigma) + (1 - y_i) \\log(1 - \\sigma)) = h_y(\\sigma)$, we get:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial f}{\\partial w_j} g'(z) h'(\\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very easy to compute the exact formula of the derivatives:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial w_j}(w, b) = x_{i,j}$\n",
    "\n",
    "$g'(z) = \\frac{e^{-z}}{(1 + e^{-z})^2}$\n",
    "\n",
    "$h'(\\sigma) = - \\frac{y_i}{\\sigma} + \\frac{1 - y_i}{1 - \\sigma}$\n",
    "\n",
    "When computing $L$, we thus need to keep in memory the values of $\\frac{\\partial f}{\\partial w_j}(w, b)$, $g'(z)$, and $h'(\\sigma)$ to be able to compute the gradient. That is what PyTorch is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch (https://pytorch.org/) is a Python package which allows you to build and train neural networks. It is based on automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import exp\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import a dataset as an example. This example has been downloaded from Kaggle: https://www.kaggle.com/adityakadiwal/water-potability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('water_potability.csv')\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.417441</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436524</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.986339</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.584087</td>\n",
       "      <td>188.313324</td>\n",
       "      <td>28748.687739</td>\n",
       "      <td>7.544869</td>\n",
       "      <td>326.678363</td>\n",
       "      <td>280.467916</td>\n",
       "      <td>8.399735</td>\n",
       "      <td>54.917862</td>\n",
       "      <td>2.559708</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.223862</td>\n",
       "      <td>248.071735</td>\n",
       "      <td>28749.716544</td>\n",
       "      <td>7.513408</td>\n",
       "      <td>393.663396</td>\n",
       "      <td>283.651634</td>\n",
       "      <td>13.789695</td>\n",
       "      <td>84.603556</td>\n",
       "      <td>2.672989</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.635849</td>\n",
       "      <td>203.361523</td>\n",
       "      <td>13672.091764</td>\n",
       "      <td>4.563009</td>\n",
       "      <td>303.309771</td>\n",
       "      <td>474.607645</td>\n",
       "      <td>12.363817</td>\n",
       "      <td>62.798309</td>\n",
       "      <td>4.401425</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ph    Hardness        Solids  Chloramines     Sulfate  Conductivity  \\\n",
       "0   8.316766  214.373394  22018.417441     8.059332  356.886136    363.266516   \n",
       "1   9.092223  181.101509  17978.986339     6.546600  310.135738    398.410813   \n",
       "2   5.584087  188.313324  28748.687739     7.544869  326.678363    280.467916   \n",
       "3  10.223862  248.071735  28749.716544     7.513408  393.663396    283.651634   \n",
       "4   8.635849  203.361523  13672.091764     4.563009  303.309771    474.607645   \n",
       "\n",
       "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0       18.436524       100.341674   4.628771           0  \n",
       "1       11.558279        31.997993   4.075075           0  \n",
       "2        8.399735        54.917862   2.559708           0  \n",
       "3       13.789695        84.603556   2.672989           0  \n",
       "4       12.363817        62.798309   4.401425           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(columns=['Potability']).to_numpy()\n",
    "y = data.Potability.to_numpy()\n",
    "N = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to nomalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(x)\n",
    "x = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to compute the loss corresponding to the first observation in the dataset. Instead of using Numpy arrays to put our data and parameters, we are going to use torch tensors, because they have properties that Numpy arrays do not have.\n",
    "\n",
    "This is the features of the first observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i = torch.torch.from_numpy(x[0, :])\n",
    "x_i = x_i.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7825,  0.5641,  0.0117,  0.5838,  0.5744, -0.7840,  1.2270,  2.1117,\n",
       "         0.8448])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the class of the first observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n"
     ]
    }
   ],
   "source": [
    "y_i = y[0]\n",
    "print(y_i.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take random values for $w$ and $b$. When creating these variables, we use the option requires_grad=True because we will later want to compute the gradient with respect to these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.rand(9, requires_grad=True)\n",
    "B = torch.rand(1, requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will be used to specify that we will want to compute the gradient with respect to the variable var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_grad(var):\n",
    "    def hook(grad):\n",
    "        var.grad = grad\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define $z = f(w, b) = x_i^T w + b$. We have $\\frac{\\partial f}{\\partial w_j} = x_{i,j}$ and $\\frac{\\partial f}{\\partial b} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = W.dot(x_i) + B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0461], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x17a7054c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.register_hook(set_grad(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define $\\sigma = g(z) = \\frac{1}{1 + e^{-z}} = g(f(w, b)) = (g \\circ f) (w, b)$. We have $g'(z) = \\frac{e^{-z}}{(1 + e^{-z})^2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1.0 / (1.0 + torch.exp(- z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8856], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we use the function torch.exp instead of numpy.exp. That is because numpy just calculate the value of $e^x$ but does not know that the derivative of $e^x$ is $e^x$. If we want to be able to use automatic differentiation, we need to use the equivalent torch function that will compute both $\\sigma(z)$ and $\\frac{\\partial \\sigma}{\\partial z}(z)$. This last value will be necessary and we will later compute the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x17b4531f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma.register_hook(set_grad(sigma))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define $L = h(\\sigma) = - (y_i \\log(\\sigma) + (1 - y_i) \\log(1 - \\sigma)) = h(g(z)) = h(g(f(w, b))) = (h \\circ g \\circ f) (w, b)$. We have $L'(\\sigma) = - (\\frac{y_i}{\\sigma} - \\frac{1 - y_i}{1 - \\sigma})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = - (y_i * torch.log(sigma) + (1 - y_i) * torch.log(1 - sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the gradient of the loss for one observation. This command compute the gradient of L with respect to all the variables for which I asked to compute the gradient, that is W, B, z, and sigma, but it does not return the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\frac{\\partial L}{\\partial \\sigma} = - (\\frac{y_i}{\\sigma} - \\frac{1 - y_i}{1 - \\sigma})$. Let us compute the result with PyTorch and using the exact mathematical formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.7379]) tensor([8.7379], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sigma.grad, - (y_i / sigma - (1 - y_i) / (1 - sigma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\frac{\\partial L}{\\partial z} = g'(z) h'(g(z))$ that is $\\frac{\\partial L}{\\partial z} = g'(z) h'(\\sigma)$. Let us compute the result with PyTorch and using the exact mathematical formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8856]) tensor([0.8856], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z.grad, (exp(-z) / ((1 + exp(-z)) ** 2.0)) * (- (y_i / sigma - (1 - y_i) / (1 - sigma))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\frac{\\partial L}{\\partial b} = \\frac{\\partial f}{\\partial b} g'(f(w, b)) h'(g(f(w, b)))$ that is $\\frac{\\partial L}{\\partial b} = \\frac{\\partial f}{\\partial b} g'(z) h'(\\sigma)$. Similarly, we have $\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial f}{\\partial w_j} g'(f(w, b)) h'(g(f(w, b)))$ that is $\\frac{\\partial L}{\\partial w_j} = \\frac{\\partial f}{\\partial w_j} g'(z) h'(\\sigma)$. Let us compute the result with PyTorch and using the exact mathematical formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9339]) tensor([0.9339], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(B.grad, 1 * (exp(-z) / ((1 + exp(-z)) ** 2.0)) * (- (y_i / sigma - (1 - y_i) / (1 - sigma))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7307,  0.5268,  0.0109,  0.5452,  0.5364, -0.7321,  1.1459,  1.9720,\n",
      "         0.7889]) tensor([ 0.7307,  0.5268,  0.0109,  0.5452,  0.5364, -0.7321,  1.1459,  1.9720,\n",
      "         0.7889], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(W.grad, x_i * (exp(-z) / ((1 + exp(-z)) ** 2.0)) * (- (y_i / sigma - (1 - y_i) / (1 - sigma))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now implement logistic regression using the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.torch.from_numpy(x)\n",
    "X = X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = torch.torch.from_numpy(y)\n",
    "Y = Y.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code for one iteration of the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(X, Y, W, B, alpha):\n",
    "    # Compute the loss for the current value of W and B\n",
    "    z = X @ W + B\n",
    "    sigma = 1.0 / (1.0 + torch.exp(- z))\n",
    "    L = torch.sum(- (Y * torch.log(sigma) + (1 - Y) * torch.log(1 - sigma)))\n",
    "    # Compute the gradient of the loss\n",
    "    L.backward()\n",
    "    # Specifically, we want the gradient with respect to W and B\n",
    "    dW = W.grad\n",
    "    dB = B.grad\n",
    "    # Update the values of W and B\n",
    "    W = W - alpha * dW\n",
    "    W.retain_grad()\n",
    "    B = B - alpha * dB\n",
    "    B.retain_grad()\n",
    "    # Return the new values of W and B\n",
    "    return (W, B, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, Y, alpha, max_iter, epsilon):\n",
    "    p = X.size()[1]\n",
    "    # We initiate W and B with random values\n",
    "    W = torch.rand(p, requires_grad=True)\n",
    "    B = torch.rand(1, requires_grad=True)\n",
    "    i_iter = 0\n",
    "    dL = 2 * epsilon\n",
    "    # We iterate until we reach the maximum number of iterations or the loss no longer decreases\n",
    "    while ((i_iter < max_iter) and (dL > epsilon)):\n",
    "        if i_iter > 0:\n",
    "            L_old = L\n",
    "        (W, B, L) = step(X, Y, W, B, alpha)\n",
    "        if i_iter > 0:\n",
    "            dL = abs((L - L_old) / L_old)\n",
    "        i_iter = i_iter + 1\n",
    "    return (W, B, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W, B, L) = logistic_regression(X, Y, 0.001, 200, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now try to make predictions on the training test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = 1.0 / (1.0 + torch.exp(- (X @ W + B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4392, 0.3959, 0.4143,  ..., 0.4750, 0.4117, 0.4629],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the torch tensor to a Numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = yhat.detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the values of the probability (between 0 and 1) into the value of the class to which each observation belongs (here 0 or 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = np.where(yhat > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now compute some classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003978120338140228\n"
     ]
    }
   ],
   "source": [
    "# True positive\n",
    "tp = np.sum((y == 1) & (yhat == 1))\n",
    "print(tp/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39930382894082544\n"
     ]
    }
   ],
   "source": [
    "# False negative\n",
    "fn = np.sum((y == 1) & (yhat == 0))\n",
    "print(fn/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014917951268025858\n"
     ]
    }
   ],
   "source": [
    "# False positive\n",
    "fp = np.sum((y == 0) & (yhat == 1))\n",
    "print(fp/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952262555942317\n"
     ]
    }
   ],
   "source": [
    "# True negative\n",
    "tn = np.sum((y == 0) & (yhat == 0))\n",
    "print(tn/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6001989060169071\n"
     ]
    }
   ],
   "source": [
    "# Accurracy (percentage of correct classifications)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011097410604192354\n"
     ]
    }
   ],
   "source": [
    "# Recall (= Sensitivity = percentage of positive value correctly classified)\n",
    "recall = tp / (tp + fn)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8181818181818182\n"
     ]
    }
   ],
   "source": [
    "# Precision (= percentage of positive predictions that were correct)\n",
    "precision = tp / (tp + fp)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021897810218978103\n"
     ]
    }
   ],
   "source": [
    "# F1\n",
    "F1 = (2 * precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Logistic regression is a nice example to start learning about automatic differentiation and PyTorch. However, if you actually want to use logistic regression for your own dataset, it is much easier to use the function already existing in ScikitLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(random_state=0).fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04101425, -0.00117745,  0.08484803,  0.0454577 , -0.01693117,\n",
       "        -0.03103009, -0.02956544,  0.01923843,  0.04572756]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.39322864])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = precision_recall_fscore_support(y, yhat, average='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666 0.004932182490752158 0.009791921664626684\n"
     ]
    }
   ],
   "source": [
    "(precision, recall, F1) = (metrics[0], metrics[1], metrics[2])\n",
    "print(precision, recall, F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
