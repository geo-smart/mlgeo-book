{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Classification and Regression\n",
    "\n",
    "\n",
    "Problems that need a *quantitative* response (numeric value) are **regression**; problems that need a *qualitative* response (boolean or category) are **classification**. Many statistical methods can be applied to both types of problems.\n",
    "\n",
    "**Binary** classification has two output classes. They usually end up being \"A\" and \"not A\". Examples are \"earthquake\" or \"no earthquake=noise\". **Multiclass** classification refers to one with more than two classes.\n",
    "\n",
    "Classification here requires that we know the labels, it is a form of *supervised learning*.\n",
    "\n",
    "## 1. Classification Algorithms\n",
    "There are several classifier algorithms, which we will summarize below before practicing.\n",
    "\n",
    "* **Logistic Regression**:\n",
    "\n",
    "* **Linear Discriminant Analysis (LDA)**: The LDA optimiziation methods produces an optimal dimensionality reductions to a decision line for classificaiton. It is based on variance reduction and has analogy to a PCA coordinate system.\n",
    "\n",
    "* **Stochastic Gradient Descent (SGD)**:\n",
    "\n",
    "* **Naive Bayes (NB)**: Simple algorithm that requires little hyper-parameters, provides interpretable results. The algorithm computes conditional probabilities and uses the product as a decision rule to maximize the probability in each class.\n",
    "\n",
    "* **K-nearest neighbors (KNN)**: Choose *K* as the numbers of nearest data points to consider. Gather each data sample and the K nearest ones, assign the class that is most represented in that group (the mode of the K labels).\n",
    "\n",
    "* **Support Vector Machine (SVM)**: Finds the hyperplanes that separate the classes with sufficient margins. The hyperplanes can be linear and more complex (kernels SVM such as radial basis function and polynomial kernels). SVM was very popular for limited training data.\n",
    "\n",
    "* **Random Forest (RF)**: Decision trees are common for prediction pipelines. *Decision tree learning* is a method to create a predictive model of trees based on the data. More on that this monday.\n",
    "\n",
    "Some classifiers can handle multi class natively (Stochastic Gradient Descent - SGD; Random Forest classification;  Naive Bayes). Others are strictly binary classifiers (Logistic Regression, Support Vector Machine classifier - SVM). \n",
    "\n",
    "\n",
    "## 2. Regression Algorithms\n",
    "\n",
    "TO ADD\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('madrona')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c2df93b363d800c8a9b94963221f1be1d8deaf6a76f83b6b9a486ad05d69583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
