{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dfc3a0d-777c-4e95-b242-8c130d51bd3d",
   "metadata": {},
   "source": [
    "# 3.3 Binary classification\n",
    "\n",
    "\n",
    " We can compare them all in one [exercise](!https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code source: Gaël Varoquaux\n",
    "#              Andreas Müller\n",
    "# Modified for documentation by Jaques Grobler\n",
    "# License: BSD 3 clause\n",
    "\n",
    "# basic tools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# classifiers from sklearns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb272d",
   "metadata": {},
   "source": [
    "### 1.1 Synthetic Data\n",
    "First, we start making new data using the scikitlearn tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d396cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data set\n",
    "X, y = make_moons(noise=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae13ea",
   "metadata": {},
   "source": [
    "Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=y, cmap='PiYG', edgecolors=\"k\");plt.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b9e5091",
   "metadata": {},
   "source": [
    "We will start with the fundamental LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# define ML\n",
    "clf = LinearDiscriminantAnalysis() \n",
    "\n",
    "# normalize data.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# split data between train and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Fit the model.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# calculate the mean accuracy on the given test data and labels.\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"The mean accuracy on the given test and labels is %f\" %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24f78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "ax = plt.subplot()\n",
    "# plot the decision boundary as a background\n",
    "DecisionBoundaryDisplay.from_estimator(clf, X, cmap='PiYG', alpha=0.8, ax=ax, eps=0.5)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='PiYG', alpha=0.6, edgecolors=\"k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeeeb95",
   "metadata": {},
   "source": [
    "The results shows a not-too bad classification, but a low confidence.\n",
    "\n",
    "Let's try a different classifer: KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e3f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ML\n",
    "K = 5\n",
    "clf= KNeighborsClassifier(K)\n",
    "\n",
    "# normalize data.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# split data between train and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Fit the model.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# calculate the mean accuracy on the given test data and labels.\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"The mean accuracy on the given test and labels is %f\" %score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9ebb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision boundary as a background\n",
    "ax = plt.subplot()\n",
    "DecisionBoundaryDisplay.from_estimator(clf, X, cmap='PiYG', alpha=0.8, ax=ax, eps=0.5)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='PiYG', alpha=0.6, edgecolors=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c8f0d",
   "metadata": {},
   "source": [
    "Now we will test to see what happens when you do not **normalize** your data before the classification. We will stretch the first axis of the data to see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77348ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data set\n",
    "X, y = make_moons(noise=0.3, random_state=0)\n",
    "X[:,0] = 10*X[:,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define ML\n",
    "K = 5\n",
    "clf= KNeighborsClassifier(K)\n",
    "\n",
    "# split data between train and test set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Fit the model.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# calculate the mean accuracy on the given test data and labels.\n",
    "score = clf.score(X_test, y_test)\n",
    "print(\"The mean accuracy on the given test and labels is %f\" %score)\n",
    "\n",
    "# plot the decision boundary as a background\n",
    "ax = plt.subplot()\n",
    "# DecisionBoundaryDisplay.from_estimator(clf, X, cmap='PiYG', alpha=0.8, ax=ax, eps=0.5)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap='PiYG', alpha=0.6, edgecolors=\"k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a9bd5",
   "metadata": {},
   "source": [
    "This drastically reduces the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4ed4b70-5e28-45c1-8bbc-d7926603d142",
   "metadata": {},
   "source": [
    "## 2. Classifier Performance Metrics\n",
    "\n",
    "In a binary classifier, we label one of the two classes as *positive*, the other class as *negative*. Let's consider *N* data samples.\n",
    "\n",
    "| True Class \\ predicted Class     | Positive            | Negative           | **Total** |\n",
    "|  -------------  |  -----------------  |  --------------- | ----- |\n",
    "| Positive        | True Positive   | False Negative | p     |\n",
    "| Negative        | False Positive  | True Negative  | n     |\n",
    "| **Total**       | p'                  | n'                 | N     |\n",
    "\n",
    "In this example, there were originally a total of $p=FN+TP$ positive labels and $n=FP+TN$ negative levels. We ended up with $p'=TP+FP$ predicted as positive and $n'=FN+TN$ predicted as negative.\n",
    "\n",
    "**True positive TP**: the number of data predicted as *positive* that were originally *positive*. \n",
    "\n",
    "**True negative TN**: the number of data predicted as *negative* that were originally *negative*.\n",
    "\n",
    "**False positive FP**: the number of data predicted as *positive* but that were originally *negative*.\n",
    "\n",
    "**False negative FN**: the number of data predicted as *negative* that were originally *positive*. \n",
    "\n",
    "**Confusion matrix:**\n",
    "Count the instances that an element of class *A* is classified in class *B*. A 2-class confusion matrix is:\n",
    "\n",
    "$ C = \\begin{array}{|cc|} TP & FN \\\\ FP  & TN \\end{array}$\n",
    "\n",
    "The confusion matrix can be extended for a multi-class classification and the matrix is KxK instead of 2x2. The best confusion matrix is one that is close to identity, with little off diagonal terms.\n",
    "\n",
    "**Other model performance metrics**\n",
    "Model peformance can be assessed with the following:\n",
    "* **Error** : the fraction of the data that was misclassified \n",
    "\n",
    "    $err = \\frac{FP+FN}{N}$  -> 0\n",
    "* **Accuracy**: the fraction of the data that was correctly classified: \n",
    "    \n",
    "    $acc = \\frac{TP+TN}{N} = 1 - err $ --> 1\n",
    "\n",
    "* **TP-rate**: the ratio of samples predicted in the *positive* class that are correctly classified:\n",
    "\n",
    "    $TPR = \\frac{TP}{TP+FN}$ --> 1\n",
    "    \n",
    "    This ratio is also the **recall** value or **sensitivity**.\n",
    "\n",
    "* **TN-rate**: the ratio of samples predicted in the *negative* class that are correctly classified:\n",
    "\n",
    "    $TNR = \\frac{TN}{TN+FP}$ --> 1\n",
    "    \n",
    "    This ratio is also the **specificity**.\n",
    "\n",
    "* **Precision**: the ratio of samples predicted in the *positive* class that were indeed *positive* to the total number of samples predicted as *positive*.\n",
    "\n",
    "    $pr = \\frac{TP}{TP+FP}$ --> 1\n",
    " \n",
    "* **F1 score**:\n",
    "\n",
    "    $F_1 = \\frac{2}{(1/ precision + 1/recall)} = \\frac{TP}{TP + (FN+FP)/2} $ --> 1.\n",
    "\n",
    "    \n",
    "The harmonic mean of the F1 scores gives more weight to low values. F1 score is thus high if both recall and precision are high.\n",
    "\n",
    "Let's print these measures from our classification using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score \n",
    "\n",
    "# Fit the model.\n",
    "y_test_pred=clf.predict(X_test)\n",
    "\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "print(\"precison, recall\")\n",
    "print(precision_score(y_test,y_test_pred),recall_score(y_test,y_test_pred))\n",
    "print(\"F1 score\")\n",
    "print(f1_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d3519a",
   "metadata": {},
   "source": [
    "A complete well-formatted report of the performance can be called using the function ``classification_report``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d701bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(f\"Classification report for classifier {clf}:\\n\"\n",
    "      f\"{classification_report(y_test, y_test_pred)}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09d8d5f3",
   "metadata": {},
   "source": [
    "**Precision and recall trade off**: increasing precision reduces recall.\n",
    "\n",
    "$precision = \\frac{TP}{TP+FP}$ \n",
    "\n",
    "$recall = \\frac{TP}{TP+FN}$ \n",
    "\n",
    "\n",
    "The classifier uses a *threshold* value to decide whether a data belongs to a class. Increasing the threshold gives higher precision score, decreasing the thresholds gives higher recall scores. Let's look at the various score values.\n",
    "\n",
    "**Receiver Operating Characteristics ROC** \n",
    "\n",
    "It plots the true positive rate against the false positive rate.\n",
    "The ROC curve is visual, but we can quantify the classifier performance using the *area under the curve* (aka AUC). Ideally, AUC is 1.\n",
    "\n",
    "![ROC curve](roc-curve-v2-glassbox.png)\n",
    "\n",
    "[source: https://commons.wikimedia.org/wiki/File:Roc-draft-xkcd-style.svg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4820c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "y_scores = clf.predict_proba(X_train)\n",
    "fpr,tpr,thresholds=roc_curve(y_train,y_scores[:,1])\n",
    "plt.plot(fpr,tpr,linewidth=2);plt.grid(True)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.plot([0,1],[0,1],'k--')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802272c7",
   "metadata": {},
   "source": [
    "We now explore the different classifiers packaged in scikit learn. We can systematically test their performance and save the precision, recall, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d83bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d3907",
   "metadata": {},
   "source": [
    "## 3. Model exploration\n",
    "\n",
    "* Explore How each of these models perform on the synthetic data. \n",
    "\n",
    "* Save in an array the precision, recall, F1 score values.\n",
    "\n",
    "* Find the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e400ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "pre=np.zeros(len(classifiers))\n",
    "rec=np.zeros(len(classifiers))\n",
    "f1=np.zeros(len(classifiers))\n",
    "for ii,iclass in enumerate(classifiers):\n",
    "    iclass.fit(X_train, y_train)\n",
    "    y_test_pred=iclass.predict(X_test)\n",
    "    pre[ii] =precision_score(y_test,y_test_pred)\n",
    "    rec[ii] =recall_score(y_test,y_test_pred)\n",
    "    f1[ii] =f1_score(y_test,y_test_pred)\n",
    "\n",
    "df=pd.DataFrame({'CLF name':names,'precision':pre,'recall':rec,'f1_score':f1})\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf1c1ab31e530e60b58e3d6ad0457a0c579c03efa8f6c28b6cdd125835b5a825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
