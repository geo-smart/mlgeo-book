{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9 Ensemble learning\n",
    "\n",
    "Ensemble learning involves combining several **weak learners** to build a **strong learner** that is more robust and generalizable. A popular example is the choice of the Random Forest algorithm that is a lot stronger than individual Decision Trees that compose the Forest.\n",
    "\n",
    "Several key advantages to ensemble learning are:\n",
    "* **Reduced Overfitting**: Ensemble methods, particularly bagging techniques like Random Forests, mitigate the risk of overfitting by combining predictions from multiple models. \n",
    "* **Improved generalization**: As a follow up from the previous point, ensemble methods may capture complex relationships more effectively and shows improved generalization on diverse data.\n",
    "* **Robustness to noise and outliers:** Ensemble methods, by aggregating predictions from multiple models, are less susceptible to the impact of individual noisy or outlier-laden data points. This results in more robust predictions, particularly in the presence of imperfect or uncertain data.\n",
    "* **Increased model stability:**  Individual models might perform well on certain subsets of the data but poorly on others. By combining diverse models, ensembles provide a more stable and reliable prediction across various scenarios and subgroups within geospatial datasets.\n",
    "\n",
    "\n",
    "Below we will"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "# explore data type\n",
    "data,y = digits[\"data\"].copy(),digits[\"target\"].copy()\n",
    "print(type(data[0][:]),type(y[0]))\n",
    "# note that we do not modify the raw data that is stored on the digits dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1797 data samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(data)# fit the model for data normalization\n",
    "newdata = scaler.transform(data) # transform the data. watch that data was converted to a numpy array\n",
    "\n",
    "# Split data into 50% train and 50% test subsets\n",
    "print(f\"There are {data.shape[0]} data samples\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, y, test_size=0.2, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Voting Classifier\n",
    "\n",
    "Aggregate the predictions of each classifier and predict the class that gets the most votes.\n",
    "\n",
    "![Voting Classifer](votingclassifier.png)\n",
    "From \"Hands on Machine Learning With Sci-kit Learn, Keras, and Tensorflow\" (Gueron).\n",
    "\n",
    "\n",
    "Next, we are going to create an ensemble of models. The models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.925"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "svc_clf = SVC() # model design\n",
    "nb_clf = GaussianNB()\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [ ('nb',nb_clf) , ('rf',rf_clf),('svc',svc_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train) # learn\n",
    "y_pred=voting_clf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with individual ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB 0.8263632119514472\n",
      "RandomForestClassifier 0.9615219421101775\n",
      "SVC 0.9793790849673202\n",
      "VotingClassifier 0.9687815126050421\n"
     ]
    }
   ],
   "source": [
    "for clf in (nb_clf,rf_clf,svc_clf,voting_clf):\n",
    "    scores = cross_val_score(clf, data, y, scoring='accuracy', cv=15)\n",
    "    print(clf.__class__.__name__,scores.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that taking the average of the predicted probability, instead of its max, is also a possibility. One can only evaluate the classifiers that output probabilities, which SVM does not do by default. Set the ``voting`` to ``soft`` to compare with the mean proba. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bagging and Pasting\n",
    "\n",
    "This approaches uses the same model algorithm but resampling on the training set. For resampling **with replacement** (bootstrap), it is called *bagging*; for resampling **without replacement**, it is called *pasting*.\n",
    "\n",
    "Several models are trained on different data, then the predictions are aggregated (*statistical mode* for classification and *average* for regression). The aggregated model tends to have a lower variance and bias, similar to what it would get if it were trained on the entire data.\n",
    "\n",
    "In the example below, you can implement BaggingClassifier in sklearn and explore by changing the number of model or bootstrapping value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy 0.9599473847106159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator = KNeighborsClassifier(),n_estimators=100,  # n_estimator is the number of models to train\n",
    "    max_samples=1000,bootstrap=True, # bootstrap is for bagging vs pasting\n",
    "    n_jobs=-1,#number of CPU cores independently used for training and prediction. Use -1 for all available score\n",
    "    )\n",
    "scores=cross_val_score(bag_clf,data,y,cv=5)\n",
    "print('mean accuracy',scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One property of the resampling replacement is that for $m$ sampled data with replacement, up to 63% of the data tends to be sampled in average, as $m$ tends to the size of the total number of samples. This means that in average, 37% of the data never gets sampled. That portion act as a testing set. To get the accuracy score on that portion of the data, we can set the argument for BaggingClassifer ``oob_score=True`` and compare the with the score from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n",
      "/Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages/sklearn/ensemble/_base.py:156: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "bag_clf = BaggingClassifier(\n",
    "    base_estimator = KNeighborsClassifier(),n_estimators=100,  # n_estimator is the number of models to train\n",
    "    max_samples=100,bootstrap=True, # bootstrap is for bagging vs pasting\n",
    "    n_jobs=-1,oob_score=True,#number of CPU cores independently used for training and prediction. Use -1 for all available score\n",
    "    )\n",
    "scores=cross_val_predict(bag_clf,X_test,y_test,cv=5) # the size of max_samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Boosting\n",
    "\n",
    "The idea behind boosting methods is to train predictors sequentially, each trying to correct its predecessor.\n",
    "\n",
    "### 3.1 AdaBoost\n",
    "The AdaBoost algorithm trains a new predictor by paying more attention (up-weighting) the bad predictions from the previous predictor. For instance, in a classification, a first predictor will be underfitting the data and misclassifying labels. The second predictor weights more strongly the data that was misclassified. The ``learning_rate`` parameters set the magnitude of that weighting.\n",
    "\n",
    "AdaBoost work on any classifier that outputs probabilities (e.g., DecisionTrees, KNN; look for the classifiers that can take the function ``predict_proba()``).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=100),n_estimators=2000,\n",
    "    algorithm='SAMME.R',learning_rate=0.01) # use the algorithm SAMME for binary classificaiton, SAMME.R for multi-class\n",
    "ada_clf.fit(X_train,y_train)\n",
    "y_pred=ada_clf.predict(X_test)\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Gradient Boosting\n",
    "\n",
    "Algorithm that sequentially fits the data and its predicted residuals:\n",
    "- Single small estimator is trained on the data (a *weak learner*)\n",
    "- Second small estimator is trained on the residuals between the data and the predicted data from the first estimator (a second *weak learner*). The residuals are getting smaller\n",
    "- A third small estimator...\n",
    "\n",
    "- The final prediction is the sum of all predictions.\n",
    "\n",
    "Gradient boosting is typically used using Decision Trees. ``n_estimators`` limits the total number of trees. When ``n_estimators`` is very large, the model will overfit the data. To regularize the training, one can find the optimal number of estimators by looking at the progressive reduction of the residuals until the residual curve flattens.\n",
    "\n",
    "The ``learning_rate`` is a hyperparameter that scales the contribution of each tree. When ``learn_rate`` is low, the GB will need more trees to git.\n",
    "\n",
    "The most popular algorithm is **XGBoost** for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.2-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 1.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy in /Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages (from xgboost) (1.11.3)\n",
      "Requirement already satisfied: numpy in /Users/marinedenolle/opt/miniconda3/envs/mlgeo/lib/python3.9/site-packages (from xgboost) (1.26.0)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "Nclass = len(np.unique(y_train))\n",
    "train = xgb.DMatrix(X_train,label=y_train)\n",
    "test = xgb.DMatrix(X_test,label=y_test)\n",
    "print(Nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'max_depth':40, # depth of the trees\n",
    "    'eta':0.3,  # learning rate\n",
    "    'objective':'multi:softmax',\n",
    "    'num_class':Nclass\n",
    "}\n",
    "epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgb.train(param,train,epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8861111111111111"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stacking\n",
    "\n",
    "Each estimator may predict a value. Instead of taking the majority vote, one can aggregate the probabilities using another estimator. The last estimator is called a **meta learning** that takes all of the predictions and fit to the labeled prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
