# 3.1 Concepts in training supervision

Slides for a general overview that combines chapter 3.1 and 3.2 can be found below


[![Class Slides](../img/ )](../slides/classificationAndRegression.pdf)

Different learning paradigms cater to various types of data and problem settings. This chapter explores the general frameworks for training models, focusing on unsupervised learning, semi-supervised learning, self-supervised learning, and reinforcement learning. Understanding these frameworks will equip you with the knowledge to select the most suitable approach for your geoscientific research.



## 1. Unsupervised Learning

Unsupervised learning involves training models on datasets without labeled outputs. The goal is to uncover hidden patterns, structures, or features within the data. Unlike supervised learning, there are no explicit target variables to guide the learning process.

### Key Techniques

* **Clustering**: Groups similar data points based on feature similarity.
    * **Examples**: K-means, Hierarchical Clustering, DBSCAN.
* **Dimensionality Reduction**: Reduces the number of features while preserving essential information.
    * **Examples**: Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE).
* **Anomaly Detection**: Identifies data points that deviate significantly from the norm.
    * **Examples**: Isolation Forest, One-Class SVM.

Examples in geosciences might be geochemical anomaly detection to Identifying areas with unusual mineral concentrations (REF); seismic facies analysis to grouping seismic data to interpret subsurface geological structures (REF), or climate pattern discovery by Uncovering patterns in climate data to understand atmospheric phenomena (REF).

### Advantages and Challenges

| Advantages | Challenges |
|------------|------------|
| No need for labeled data, which can be costly or impractical to obtain. | Interpretation of results can be subjective. |
| Useful for exploratory data analysis. | Model evaluation is less straightforward due to the absence of ground truth. |

## 2. Semi-Supervised Learning

Semi-supervised learning combines a small amount of labeled data with a large amount of unlabeled data during training. It leverages the unlabeled data to improve model performance, especially when obtaining labels is expensive or time-consuming.

### Key Approaches

* **Self-Training**: The model is initially trained on labeled data, then used to predict labels for unlabeled data, which are added to the training set iteratively.
* **Co-Training**: Two or more models are trained on different subsets or views of the data, each helping to label the unlabeled data for the other.
* **Graph-Based Methods**: Use graph structures to represent data points and propagate labels through connections.

Appliations in geosciences may be enhancing land cover mapping were few ground truth labels exist (REF), predicting minerals from abundant but unlabeled geophysical data sets (REF), or assessing environmental hazards such as landslides and floors with limited labeled incidents (REF).

| Advantages                                      | Challenges                                                   |
|-------------------------------------------------|--------------------------------------------------------------|
| Improves model accuracy with less labeled data. | Risk of propagating incorrect labels.                        |
| Cost-effective when labeling is expensive.      | Model performance heavily depends on the quality of the initial labeled dataset. |

## 3. Self-Supervised Learning

Self-supervised learning is a subset of unsupervised learning where the data itself provides the supervision. Models are trained to predict parts of the data from other parts, creating surrogate tasks that generate labels from the dataâ€™s structure.

### Key Concepts

* **Pretext Tasks**: Artificial tasks designed to train models on inherent data structures.
    * **Examples**: Predicting missing data, solving jigsaw puzzles with image patches, temporal order verification.
* **Representation Learning**: The model learns meaningful representations that can be transferred to downstream tasks.

Examples in the geosciences might be training models to reconstruct missing portions of seismic signals (REF) or time series forecasting such as predicting future geophysical measurements based on past data (REF). 


| Advantages                                      | Challenges                                                   |
|-------------------------------------------------|--------------------------------------------------------------|
| Utilizes vast amounts of unlabeled data.        | Designing effective pretext tasks can be non-trivial.        |
| Produces robust feature representations.        | The learned representations may not always transfer well to specific tasks. |

## 4 Reinforcement Learning

Reinforcement learning (RL) is a learning paradigm where an agent interacts with an environment to achieve a goal. The agent learns to make decisions by performing actions and receiving feedback in the form of rewards or penalties.

### Key Components

* **Agent**: The entity making decisions.
* **nvironment**: The context in which the agent operates.
* **State**: A representation of the current situation.
* **Action**: Choices available to the agent.
* **Reward Signal**: Feedback received after an action, guiding the learning process.
* **Policy**: The strategy used by the agent to decide actions based on states.
* **Value Function**: Estimates how good it is to be in a certain state or to perform an action.

### Key Algorithms

* **Q-Learning**: Learns the value of action-state pairs to derive an optimal policy.
* **Deep Q-Networks (DQN)**: Uses neural networks to approximate Q-values in high-dimensional spaces.
* **Policy Gradients**: Optimizes the policy directly by maximizing expected rewards.
* **Actor-Critic Methods**: Combines value-based and policy-based approaches for improved learning.

<!-- Examples in geosciences might be (REF) that determined optimal drilling strategies to maximize yield in resouce extraction, or (REF) that developed policies for sustainable land use and conservation efforts, and (REF) that developed a model to guide drones in mapping or surveying tasks for automomous exploration. -->

| Advantages                                      | Challenges                                                   |
|-------------------------------------------------|--------------------------------------------------------------|
| Suitable for sequential decision-making problems. | Requires extensive interactions with the environment.        |
| Capable of learning complex behaviors.           | Computationally intensive and may suffer from convergence issues. |

## 6. Active Learning

Active learning is a machine learning paradigm where the model actively queries the user or some other information source to obtain the desired outputs at new data points. This approach is particularly useful when labeled data is scarce or expensive to obtain, as it aims to achieve high accuracy with minimal labeled data.

### Key Components

* **Query Strategy**: The method used to select which data points to label next.
	* **Examples**: 
		* **Uncertainty Sampling**: Selecting seismic data points where the model is least certain about the subsurface structure.
		* **Query by Committee**: Using multiple models to identify geological samples with the highest disagreement in mineral classification.
		* **Expected Model Change**: Choosing geophysical measurements that would most significantly alter the current model of a mineral deposit if labeled.
* **Oracle**: The source of the true labels, often a human annotator or an expert.
	* **Examples**: 
		* A geologist providing labels for rock samples.
		* A climate scientist annotating weather pattern data.
* **Stopping Criterion**: The condition that determines when the active learning process should stop.
	* **Examples**: 
		* Fixed budget: Stopping after labeling a predetermined number of seismic survey points.
		* Performance threshold: Halting when the model's accuracy in predicting mineral deposits reaches a satisfactory level.
		* Convergence criteria: Ceasing the process when additional labeled geophysical data no longer significantly improves the model.

### Key Algorithms

* **Uncertainty Sampling**: Selects data points where the model is least certain about the predictions.
* **Query by Committee**: Uses a committee of models to select data points where there is maximum disagreement among the models.
* **Expected Model Change**: Chooses data points that would result in the greatest change to the current model if labeled and added to the training set.
* **Density-Weighted Methods**: Combines uncertainty with the density of data points to ensure diverse and informative samples are selected.

### Advantages and Challenges

| Advantages                                      | Challenges                                                   |
|-------------------------------------------------|--------------------------------------------------------------|
| Reduces the amount of labeled data needed.      | Requires an effective query strategy.                        |
| Cost-effective when labeling is expensive.      | The quality of the oracle can impact performance.            |
| Can improve model performance significantly.    | Determining the stopping criterion can be difficult.         |




## 7. Comparative Overview


| Learning Paradigm       | Typical usage                                                                 | Data Status                          | Model Evaluation                                                                 |
|-------------------------|-----------------------------------------------------------------------------|--------------------------------------------|----------------------------------------------------------------------------------|
| **Supervised**   | Automate workflows for data mining or develop surrogate models | Labeled                              | silhouette score or through domain-specific interpretation. |
| **Unsupervised**   | Explore or understand the dataâ€™s structure. | Unlabeled                              | silhouette score or through domain-specific interpretation. |
| **Semi-Supervised**| automating workflows  | Limited labeled data and mostly unlabeled data            | Performance measured on labeled validation sets.                                 |
| **Self-Supervised**| Feature representations learning | Unlabeled data with inherent structure   | Indirectly evaluated by the performance on downstream tasks after fine-tuning.    |
| **Reinforcement**  | Applicable when decisions need to be made sequentially with feedback from interactions. | An environment where the agent can interact and receive rewards | Cumulative rewards or achieving the desired level of performance. |
| **Active**  | Continuous model training to improve performance | mostly unlabeled, a few labeled data | Performance measured on labeled validation sets, focusing on the efficiency of the labeling process. |


## Key Takeaways:

* **Framework Selection**: Align your choice of learning framework with the nature of your data and the problem at hand.
* **Data Utilization**: Maximize the value of available data, whether labeled or unlabeled, through appropriate learning paradigms.
* **Continuous Learning**: Stay abreast of advancements in machine learning to incorporate new techniques and improve model performance.
