{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec9a3efc-5ec8-4b1a-8669-7344fd6df78b",
   "metadata": {},
   "source": [
    "# 4.3 Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a22d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "387b3420",
   "metadata": {},
   "source": [
    "Just like MLPs, each neuron in a CNN receives some inputs, performs a dot product, and outputs predictions that will be sent to an activation function. CNNs are designed to have images as inputs. When using fully connected layers, images have too many samples and a fully connected layer would have too many trainable parameters. CNN layers are not 1D hidden layers, they are volume of neurons.\n",
    "\n",
    "![Convolutional Kernel](../img/cnn2.jpeg)\n",
    "Figure: A ConvNet arranges its neurons in three dimensions (width, height, depth), as visualized in one of the layers.\n",
    "\n",
    "\n",
    "The classic sequencing of a CNN (or a block) is: Convolutional Layer, Activation Layer, Pooling Layer, Fully Connected Layer.\n",
    "\n",
    "\n",
    "<!-- <img src=\"figures/Convolution.png\" alt=\"cnn\" style=\"width: 400px;\"/> -->\n",
    "## 1.1 Anatomy of the Conv layer\n",
    "* The **input** \"feature map\" is the input data for a given layer. It is 3D for a 2D convolution. The three dimensions are: *height*, *width*, *depth/channels*. For instance, width and heights are the 2D images (e.g. 32x32 pixels), the depth can be the different RGB (1 image per color R, G, B). For *1D convolution*, the input data should be ordered as the following **(batch size, number of channels, length of signal)**. For *2D convolutions*, the input size is **(batch size, number of channels, pixel height, pixel width)**.\n",
    " \n",
    "* The **filters** are the **convolution kernels** are the dimensionality of the output space. In general, the number of filters is greater than the input number of channels: the network heights and widths usually decrease throughout the networks, thus increasing the depth or number of filters does not increase the complexity too much. \n",
    "* The **kernel_size** is a list of 2 integers that specifies the height and width of the 2D convolution kernel. If both integers are equal, just use a single integer value. The kernel is a filter that has weights to apply on the input values, each kernel map as 1 bias. Let's take an example of a 3x3 input feature map, a 2x2 kernel size, a 2x2 output feature map. The highlighted part of the map in blue are those we focus on:\n",
    "\n",
    "![Convolution Kernel](../img/convolution.png)\n",
    " <!-- width=\"600\"></center> -->\n",
    "Convolution with kernel window (Fig. 6.2.1 from Dive into Deep Learning).\n",
    "\n",
    "In the example above, we perform the following operation: the top-left output value is = 0×0+1×1+3×2+4×3=19. Then we repeat the process until all elements of the output map are filled. We also repeat this for each filter. Usually prefer using small but many kernels/filters.\n",
    "\n",
    "\n",
    "![Convolutional Kernel](../img/CNN_StanfordCS230.gif)\n",
    "\n",
    "\n",
    "\n",
    "* The **stride** is the step that the convolution skips when being applying the filters/kernels. Small strides work better in practice. It is one way to reduce the feature map, but the most popular choice for this is to use ***maxpooling***.\n",
    "\n",
    "\n",
    "* The **padding_mode** is set to either ``same`` or ``valid`` depending on whether the edges of the feature map are extended and filled with zeros (same) to fit the total length of the kernel size and stride, or whether they are ignored (valid). Prefer to use ``same`` especially for the hidden conv layers to avoid losing data and feature knowledge.\n",
    "\n",
    "\n",
    "The convolution can be 1D or 2D depending on the array input:\n",
    "\n",
    "When the input is a single dimension array (vector, time series), use a [Conv1D layer](!https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d).\n",
    "\n",
    "When the input is a 2D image (2D array) or a time series with multiple channels (example of a seismogram with 3 components), use [Conv2D layers](!https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d).\n",
    "\n",
    "Here are examples of a 32x32 image with three channels (RGB) sent to a 64 output channels/filters with a size of kernels of 6 pixels. The padding is \"same\" such that the edges of the feature maps are filled with zeros. We write the function in Pytorch.\n",
    "\n",
    "\n",
    "A great lecture about CNN is one done at Stanford, [CS230](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408aa658-75cb-4043-ab3b-864956d1c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch version\n",
    "torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9937a2a-8238-4f30-b126-372a940c9e34",
   "metadata": {},
   "source": [
    "## 1.2 Pooling layers\n",
    "\n",
    "**MaxPooling** layers are downsampling layers. It ouputs the max value of each channel of windows in a feature map. Downsampling reduces the size of the feature-map, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover). The pooling size is the factor of reduction in the layer size.\n",
    "\n",
    "\n",
    "![Max Pooling](../img/max_pooling.png)\n",
    "\n",
    "Maximum pooling (Fig. 6.5.1 from Dive into Deep Learning).\n",
    "\n",
    "<!-- 114 / 2 = 57 and 80 / 2 = 40 -->\n",
    "\n",
    "**General pooling** are other functions, such as average pooling or even L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to the max pooling operation, which has been shown to work better in practice.\n",
    "\n",
    "\n",
    "Pooling is designed to reduce the complexity and model size in order to deal with overfitting and computational expense. Some argue that striding is sufficient. Some also found that auto-encoders and generative adversarial networks perform better without pooling.\n",
    "\n",
    "\n",
    "## 1.3 Other notes\n",
    "\n",
    "In the convolutional layer, the neurons are not connected to every part of the input data.\n",
    "\n",
    "A dense layer learns global patterns. A convolution layer learns local patterns. Because of that, CNNs are **translation invariant** as they pick part of the image of time series and generalize the learning elsewhere. CNNs learn **hierarchical patterns**: a first layer learns a local pattern, a second layer combines the local features to create a broader scale feature.\n",
    "\n",
    "\n",
    "\n",
    "# 2 Practice on LeNet-5 network\n",
    "\n",
    "We will create a CNN LeNet architecture (LeCun et al, 1998) to classify images from the fashion MNIST data. We will write it in Keras/Tensorflow and in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865611e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import load_digits,fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import transforms, ToTensor, Compose,Normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets\n",
    "import os\n",
    "\n",
    "dataset = datasets.MNIST(root=\"./\",download=True,\n",
    " transform=Compose([ToTensor(),Normalize([0.5],[0.5])]))\n",
    "L=len(dataset)\n",
    "Lt = int(0.8*L)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [Lt,L-Lt])\n",
    "loaded_train = DataLoader(train_set, batch_size=50)\n",
    "loaded_test = DataLoader(val_set, batch_size=50)\n",
    "print(loaded_train)\n",
    "\n",
    "X, y = next(iter(loaded_train))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d280968e",
   "metadata": {},
   "source": [
    "### 2.1 Create the model\n",
    "\n",
    "We will create a CNN LeNet-5 architecture (LeCun et al, 1998) that is a sequential stack of 3 convolutional layers, 2 fully connected layers. There are several graphical representations of networks that we often find in the literature, with a few examples below.\n",
    "\n",
    "![LeNet](../img/lenet.svg)\n",
    "LeNet-5 architecture\n",
    "\n",
    "![Le-net](../img/lenet-vert.svg)\n",
    "\n",
    "Using words, we can see that the CNN is composed of an input map of size 28x28 pixels, and the images are in gray scales so there is a single channel. It is followed by a convolution layer of size 28x28 and depth 6 (# of channels) and kernel sizes of 5x5, a pooling layer of size 2 - stride 2, a conv layer of depth 6 (or 6 channels) and kernel size 5x5, another pooling layer of size 2 - stride 2, and then 3 fully connected (dense) layers of respective sizes 120, 84, 10. The activation functions in the original LeNet-5 were sigmoids and the last activation function was a Gaussian function,  which we replaced with softmax. One can test the role of activation functions by changing them to ReLu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dcef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation in Keras\n",
    "# model = keras.Sequential([\n",
    "# # Must define the input shape in the first layer of the neural network\n",
    "# keras.layers.Conv2D(filters=6, kernel_size=5, padding='same', activation='sigmoid', input_shape=(28,28,1)),\n",
    "# keras.layers.AveragePooling2D(2), # you could replace with MaxPooling2D\n",
    "# keras.layers.Conv2D(filters=16, kernel_size=5, padding='same', activation='sigmoid'),\n",
    "# keras.layers.AveragePooling2D(2),# you could replace with MaxPooling2D\n",
    "# keras.layers.Flatten(),\n",
    "# keras.layers.Dense(120, activation='sigmoid'),\n",
    "# # keras.layers.Dropout(0.5),\n",
    "# keras.layers.Dense(84, activation='sigmoid'),\n",
    "# # keras.layers.Dropout(0.5),\n",
    "# keras.layers.Dense(10, activation='softmax')])\n",
    "# # Take a look at the model summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17ea87-9855-4d30-9093-7f3dab694347",
   "metadata": {},
   "source": [
    "Implementation in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c493aa-e8b3-4449-a45f-07f39f74ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410b406-0ee0-4748-8937-e83d9892d644",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lenet= torch.nn.Sequential(Reshape(), nn.Conv2d(1, 6, kernel_size=5,\n",
    "                                               padding=2), nn.Sigmoid(),\n",
    "                          nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                          nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
    "                          nn.AvgPool2d(kernel_size=2, stride=2), nn.Flatten(),\n",
    "                          nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "                          nn.Linear(120, 84), nn.Sigmoid(), nn.Linear(84, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26b786-5d3a-45ca-a276-94365a3e9099",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(size=(1, 1, 28, 28), dtype=torch.float32)\n",
    "print('Initial input shape: \\t', X.shape)\n",
    "for layer in model_lenet:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__, 'output shape: \\t', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aaf457",
   "metadata": {},
   "source": [
    "### 2.2 Prepare training\n",
    "Choose the training parameters\n",
    "* error metric: accuracy\n",
    "* loss function: *crossentropy* for multiclassification.\n",
    "* batch size, \n",
    "* the number of epochs (iterations). \n",
    "* Optimizer: Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44abd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.005# learning rate lr\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_lenet.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Set fixed random number seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae2d2421",
   "metadata": {},
   "source": [
    "### 2.3 Train the model\n",
    "Plot the accuracy scores as a function of epochs to see how well we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_epochs, trainloader, testloader=None,learning_rate=0.001 ):\n",
    "\n",
    "    os.makedirs('lenet_checkpoint',exist_ok=True)\n",
    "    # Define loss and optimization method\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # # Save loss and error for plotting\n",
    "    loss_time = np.zeros(n_epochs)\n",
    "    accuracy_time = np.zeros(n_epochs)\n",
    "\n",
    "    # # Loop on number of epochs\n",
    "    for epoch in range(n_epochs):\n",
    "    #     # Initialize the loss\n",
    "        running_loss = 0\n",
    "    #     # Loop on samples in train set\n",
    "        for data in trainloader:\n",
    "    #         # Get the sample and modify the format for PyTorch\n",
    "            inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.long()\n",
    "    #         # Set the parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "    #         # Propagate the loss backward\n",
    "            loss.backward()\n",
    "    #         # Update the gradients\n",
    "            optimizer.step()\n",
    "    #         # Add the value of the loss for this sample\n",
    "            running_loss += loss.item()\n",
    "    #     # Save loss at the end of each epoch\n",
    "        loss_time[epoch] = running_loss/len(trainloader)\n",
    "\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict()\n",
    "                        }\n",
    "        \n",
    "        f_path = './lenet_checkpoint/checkpoint.pt'\n",
    "        torch.save(checkpoint, f_path)\n",
    "        \n",
    "\n",
    "        \n",
    "    #     # After each epoch, evaluate the performance on the test set\n",
    "        if testloader is not None:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "    #         # We evaluate the model, so we do not need the gradient\n",
    "            with torch.no_grad(): # Context-manager that disabled gradient calculation.\n",
    "    #             # Loop on samples in test set\n",
    "                for data in testloader:\n",
    "    #                 # Get the sample and modify the format for PyTorch\n",
    "                    inputs, labels = data[0], data[1]\n",
    "                    inputs = inputs.float() \n",
    "                    labels = labels.long()\n",
    "    #                 # Use model for sample in the test set\n",
    "                    outputs = model(inputs)\n",
    "    #                 # Compare predicted label and true label\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "    #         # Save error at the end of each epochs\n",
    "            accuracy_time[epoch] = 100 * correct / total\n",
    "    \n",
    "    #     # Print intermediate results on screen\n",
    "        if testloader is not None:\n",
    "            print('[Epoch %d] loss: %.3f - accuracy: %.3f' %\n",
    "              (epoch + 1, running_loss/len(trainloader), 100 * correct / total))\n",
    "        else:\n",
    "            print('[Epoch %d] loss: %.3f' %\n",
    "              (epoch + 1, running_loss/len(trainloader)))\n",
    "\n",
    "    # # Save history of loss and test error\n",
    "    if testloader is not None:\n",
    "        return (loss_time, accuracy_time)\n",
    "    else:\n",
    "        return (loss_time)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a82827",
   "metadata": {},
   "outputs": [],
   "source": [
    "(loss, accuracy) = train(model_lenet, 3,loaded_train, loaded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f48763-9645-4b18-a18c-03dbe916d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "print(len(loss))\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color=color)\n",
    "ax1.plot(np.arange(1, len(loss)+1), loss, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Correct predictions', color=color)\n",
    "ax2.plot(np.arange(1, len(accuracy)+1), accuracy, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf926f36",
   "metadata": {},
   "source": [
    "# 3. Example on seismic data\n",
    "\n",
    "In this class, we will use a simplified version of ConvNetQuake (Perol et al, 2018). The network was designed as a classification algorithm that detects seismic events and assigns their location in spatial clusters. The earthquakes from a previously known earthquake catalog were clustered using k-means. We will use the two seismic station seismograms already labeled as \"earthquakes\" or \"noise\" to perform.\n",
    "\n",
    "![ConvNetQuake](../img/ConvNetQuake.jpg)\n",
    "<!-- <img src=\"figures/ConvNetQuake.jpg\" alt=\"ConvNetQuake\" style=\"width: 400px;\"/> -->\n",
    "\n",
    "\n",
    "### 3.1 read the data\n",
    "Download the data and place it in a \"data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "wget.downloads(\"https://www.dropbox.com/s/vi9gmjy8d4zd5jv/templates_029.h5?dl=1\")\n",
    "os.replace(\"templates_029.h5\", \"../../data/templates_029.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0f48ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load OK029 template data:\n",
    "with h5py.File(\"../../data/templates_029.h5\", \"r\") as f:\n",
    "    eq1 = np.asarray(f['earthquakes']);neq1=eq1.shape[0]\n",
    "    no1 = np.asarray(f[\"noise\"])\n",
    "\n",
    "\n",
    "# # load OK027 template data:\n",
    "# with h5py.File(\"./data/templates_027.h5\", \"r\") as f:\n",
    "#     eq2 = np.asarray(f['earthquakes'])\n",
    "#     no2 = np.asarray(f[\"noise\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5205181",
   "metadata": {},
   "source": [
    "### 3.2 Prep the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db75f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  allocate memory\n",
    "quakes=np.zeros(shape=(eq1.shape[0],1000,3),dtype=np.float32)\n",
    "noise=np.zeros(shape=(no1.shape[0],1000,3),dtype=np.float32)\n",
    "# quakes2=np.zeros(shape=(eq2.shape[0],1000,3),dtype=np.float32)\n",
    "# noise2=np.zeros(shape=(no2.shape[0],1000,3),dtype=np.float32)\n",
    "\n",
    "# Normalize the seismograms to their peak amplitudes\n",
    "for iq in range(eq1.shape[0]):\n",
    "    for ic in range(3):\n",
    "        if np.max(np.abs(eq1[iq,ic,:]))>0:\n",
    "            quakes[iq,:,ic]=eq1[iq,ic,:]/np.max(np.abs(eq1[iq,ic,:]))\n",
    "            \n",
    "for iq in range(no1.shape[0]):\n",
    "    for ic in range(3):\n",
    "        if np.max(np.abs(no1[iq,ic,:]))>0:\n",
    "            noise[iq,:,ic]=no1[iq,ic,:]/np.max(np.abs(no1[iq,ic,:]))\n",
    "\n",
    "# for iq in range(eq2.shape[0]):\n",
    "#     for ic in range(3):\n",
    "#         if np.max(np.abs(eq2[iq,ic,:]))>0:\n",
    "#             quakes2[iq,:,ic]=eq2[iq,ic,:]/np.max(np.abs(eq2[iq,ic,:]))\n",
    "            \n",
    "# for iq in range(no12.shape[0]):\n",
    "#     for ic in range(3):\n",
    "#         if np.max(np.abs(no2[iq,ic,:]))>0:\n",
    "#             noise2[iq,:,ic]=no2[iq,ic,:]/np.max(np.abs(no2[iq,ic,:]))\n",
    "\n",
    "# select data that is strictly positive and finite\n",
    "iq1=np.where( ( np.abs(quakes[:,0,0])>0)&(np.isfinite(quakes[:,0,0])))[0]\n",
    "# iq2=np.where( (np.abs(quakes2[:,0,0])>0)&(np.isfinite(quakes2[:,0,0])))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b034e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label & data\n",
    "y = np.concatenate((np.ones(len(iq1)+len(iq2),dtype=np.int),np.zeros(len(iq1)+len(iq2),dtype=np.int))) # 0 for noise, 1 for event\n",
    "# X = np.zeros(shape=(len(train_labels),1000,3,1))\n",
    "X= np.concatenate((quakes[iq1,:,:],quakes2[iq2,:,:],noise[iq1,:,:],noise2[iq2,:,:]),axis=0)\n",
    "X=X[...,None]# add that depth/channel dimension\n",
    "\n",
    "nlabels=2 # = len(np.unique(y))\n",
    "\n",
    "# Split train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73d6a00",
   "metadata": {},
   "source": [
    "### 3.3 Define ML model\n",
    "ConvNetQuake is a simple stack of 8 conv2d layers with 32 channels, stride of 2 kernel size of 3x3, ReLu activation functions, padding is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b7a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac888ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ebbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b273ad1-a91e-4dd8-a508-f45f024fe374",
   "metadata": {},
   "source": [
    "# 4. How to read and recode published networks\n",
    "\n",
    "Let us say that you read a research paper explaining the architecture of the convolutional neural network used by the authors to carry out their data analysis. How will you try to reproduce their results? They do not provide a github!\n",
    "\n",
    "Let us look at the following paper:\n",
    "\n",
    "Rouet-Leduc, B., Hulbert, C., McBrearty, I. W., Johnson, P. A. (2020). Probing slow earthquakes with deep learning. Geophysical Research Letters, 47, e2019GL085870. [https://doi.org/10.1029/2019GL085870](https://doi.org/10.1029/2019GL085870).\n",
    "\n",
    "![CNN RouetLeDuc 2020](../img/cnn_rouet-leduc.png)\n",
    "<!-- <img src=\"figures/cnn_rouet-leduc.png\" width=\"600\"> -->\n",
    "Schematic of the CNN and its architecture (Figure 1 from Rouet-Leduc et al. (2020)\n",
    "\n",
    "* **Batch Normalization** => unclear from the paper, but this seems to be the normalization of the data\n",
    "* **Dropout** => unclear from the paper what this is\n",
    "* **Input** Spectrogram = Image with 129 x 95 x1 pixels\n",
    "* **Conv2D** convolution is has a kernel size of 16x16 feature map of size 114x80 is depth 32 (# of channels), activation is ReLU (found in the supplementary material)\n",
    "* **Maxpooling** of size 2\n",
    "* **Dropout** 5%, found in the supplementary material\n",
    "* **Conv2D** of kernel size 8 x 8, depth 64\n",
    "* **Maxpooling** of size 2\n",
    "* **Dropout** 5%, found in the supplementary material\n",
    "* **Full Connected - Dense layers** with 36608 neurons (found in the supplementary material)\n",
    "* **Full Connected - Dense layers** with 10 neurons (found in the supplementary material)\n",
    "* **Full Connected - Dense layers** with 1 neuron, sigmoid activation function (found in the supplementary material)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd53a30",
   "metadata": {},
   "source": [
    "## 5. Tuning CNN networks\n",
    "\n",
    "There are many hyperparameters and model choices to make:\n",
    "* training: learning rate, optimizer, batch_size, loss functions, regularization\n",
    "* architecture: number of layers, depth of kernels, activation functions, batch normalization\n",
    "\n",
    "One can treat the hyperparameter search as an optimization problem. In fact, there is an entire research field about **Network Architecture Search**. One can implement this by performing a **grid search** over the model architecture hyperparameter and picking the best performing model.\n",
    "\n",
    "Keras tuner (https://keras-team.github.io/keras-tuner/) can be used to randomize the grid search.\n",
    "\n",
    "<!-- http://caffe.berkeleyvision.org/model_zoo.html -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4d5a0-1ccf-48b4-b883-14c97fe481c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=16),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size=2),\n",
    "                            nn.Dropout(0.05),\n",
    "                            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=8),\n",
    "                            nn.ReLU(),\n",
    "                            nn.MaxPool2d(kernel_size=2),\n",
    "                            nn.Dropout(0.05),\n",
    "                            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4),\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(36608, 10),\n",
    "                            nn.Sigmoid(),\n",
    "                            nn.Linear(10, 1),\n",
    "                            nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06edc7-41b3-431c-8a2f-57973653aaeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf1c1ab31e530e60b58e3d6ad0457a0c579c03efa8f6c28b6cdd125835b5a825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
