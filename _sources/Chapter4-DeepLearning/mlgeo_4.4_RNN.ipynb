{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4  Recurrent Neural Networks: Processing sequences\n",
    "\n",
    "They are used for time series predictions. Regular dense networks can also do it, and CNNs can also work for very long time series.\n",
    "A recurrent neuron  receives an input *and* the output from the neuron at the previous time step. Because each neuron learns from the previous time step, it has *memory*; but these simple cells have relatively short memory (10 cells about).\n",
    "RNNs take in a sequence and output a sequence.\n",
    "\n",
    "\n",
    "![RNN](../img/rnn.svg)\n",
    "\n",
    "From D2DL: example of an RNN with a hidden state. \n",
    "The RNN takes the multiplication of the weights and the data plus other weights with the hidden states to the next layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Synthetic data**\n",
    "\n",
    "Let's create a synthetic time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size,n_steps):\n",
    "    f1,f2,off1,off2=np.random.rand(4,batch_size,1)\n",
    "    t = np.linspace(0,1,n_steps)\n",
    "    y = 0.5*np.sin( (t-off1)*(f1*10+10) ) # first wave\n",
    "    y += 0.5*np.sin( (t-off2)*(f2*20+20) ) # second wave\n",
    "    y += 0.3* (np.random.rand(batch_size,n_steps)-0.5) # noise\n",
    "    return y[...,np.newaxis].astype(np.float32) \n",
    "# here we added a dimension to the output time series because most ML algorithms can be multidimensional, but here we are just doing a single time series.\n",
    "\n",
    "# we generate 10k time series of 51 points.\n",
    "n_steps=50\n",
    "y = generate_time_series(10000,n_steps+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[5000,:]);plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train-validation-test split**\n",
    "\n",
    "In forecasting problem, we do not want to shuffle the training and test set since we want to make sure test is a prediction from the past (training).\n",
    "\n",
    "The training data are time series of 50 points, the \"label\" or \"model output\" is the **last value** of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = y[:7000,:n_steps],y[:7000,-1] \n",
    "x_val,y_val = y[7000:9000,:n_steps],y[7000:9000,-1] \n",
    "x_test,y_test = y[9000:,:n_steps],y[9000:,-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_steps),x_train[5000,:],'b')\n",
    "plt.plot(51,y_train[5000],'r+')\n",
    "plt.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can use the past values to predict the state, called *naive forecasting*:\n",
    "```python\n",
    "y_pred=x_val[:,-1]\n",
    "```\n",
    "\n",
    "Or we can use a fully connected network and predict the value as a MLP regression:\n",
    "\n",
    "```python\n",
    "model.keras.models.Sequential([keras.layers.Flatten(input_shape=[50,1]),\n",
    "                              keras.layers.Dense(1)])\n",
    "```\n",
    "They don't do too bad in this problem. The simple block is a ``simpleRNN``. The simplest recurrent neuron is a ``SimpleRNN(1,input_shape=[None,1])`` that takes any input scalar since it can process any number of time steps. The default activation function is ``tanh``. To return a time series, and not its final output, you need to set ``return_sequences=True``. It turns out that the simplest and single recurrent neuron won't work. So we stack several simpleRNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,input_shape=[None,1],return_sequences=True),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.SimpleRNN(1)\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "history=model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=20, batch_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,input_shape=[None,1],return_sequences=True),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "history=model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=20, batch_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.xlabel('epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast of several steps ahead: how far can you predict the future?\n",
    "We will try and predict 10 steps ahead. The early part of the forecast will be a lot better than the later part of the forecast as uncertainties increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate 10k time series of 51 points.\n",
    "n_steps=50\n",
    "x = generate_time_series(10000,n_steps+10)\n",
    "y=np.empty((10000,n_steps,10))\n",
    "for step_ahead in range(1,10+1):\n",
    "    y[:,:,step_ahead-1]=x[:,step_ahead:step_ahead+n_steps,0]\n",
    "\n",
    "    \n",
    "x_train=x[:7000,:n_steps]\n",
    "x_val=x[7000:9000,:n_steps]\n",
    "x_test=x[9000:,:n_steps]\n",
    "\n",
    "y_train=y[:7000]\n",
    "y_val=y[7000:9000]\n",
    "y_test=y[9000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,input_shape=[None,1],return_sequences=True),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "history=model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=20, batch_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "print(y_pred.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_steps+10),x[9000,:])\n",
    "plt.plot(np.arange(n_steps),x_test[0,:])\n",
    "plt.plot(np.arange(10)+n_steps,y_pred[0,-1,:],'+')\n",
    "plt.legend(('Truth','past','future'))\n",
    "plt.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with RNNs and solutions\n",
    "\n",
    "Simple RNNs have issues during the training with backpropagation that gradients may become too small and that the model no longer updates during training. This is called the **vanishing gradient** problem.\n",
    "\n",
    "To remedy this, the algorithm \"LSTM\" introduces a memory-cell and gating to allow and reset the values and avoid vanishing gradients.\n",
    "\n",
    "\n",
    "### 2. LSTM\n",
    "Long-Short Term Memory are (somewhat complicated) cells that aims to solve the memory loss issue.\n",
    "\n",
    "![LSTM](../img/lstm-2.svg)\n",
    "An LSTM combines hidden state from the previous layers, the memory of the internal state, and the input data to output the current hidden and internal states.\n",
    "<!-- <img src=\"./figures/lstm.png\" alt=\"lstm\" style=\"width: 400px;\"/> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "    keras.layers.LSTM(20,input_shape=[None,1],return_sequences=True),\n",
    "    keras.layers.LSTM(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "history=model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=20, batch_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "plt.plot(np.arange(n_steps+10),x[9000,:])\n",
    "plt.plot(np.arange(n_steps),x_test[0,:])\n",
    "plt.plot(np.arange(10)+n_steps,y_pred[0,-1,:],'+')\n",
    "plt.legend(('Truth','past','future'))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf1c1ab31e530e60b58e3d6ad0457a0c579c03efa8f6c28b6cdd125835b5a825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
