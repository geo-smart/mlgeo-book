{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4  Recurrent Neural Networks: Processing sequences\n",
    "\n",
    "They are used for time series ***forecast***. Regular dense networks can also do it, and CNNs can also work for very long time series.\n",
    "\n",
    "A recurrent neuron  receives an input *and* the output from the neuron at the previous time step. Because each neuron learns from the previous time step, it has *memory*; but these simple cells have relatively short memory (10 cells about).\n",
    "RNNs take in a sequence and output a sequence.\n",
    "\n",
    "\n",
    "![RNN](../img/rnn.svg)\n",
    "\n",
    "From D2DL: example of an RNN with a hidden state. \n",
    "The RNN takes the multiplication of the weights and the data plus other weights with the hidden states to the next layer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In time series forecasting, the goal is to predict future values based on historical data. Two important concepts in this process are context data and prediction length.\n",
    "\n",
    "1. **Context Data** refers to the historical data used by the forecasting model to make predictions. This data provides the context or background information needed to understand the patterns, trends, and seasonality in the time series. The length of the context data is often referred to as the look-back window or input window.\n",
    "2. **Prediction length** refers to the number of future time steps the model aims to predict. This is also known as the ***forecast horizon***. The number of future time steps the model predicts. For example, if the forecast horizon is 5, the model predicts the next 5 time steps.\n",
    "\n",
    "**Synthetic data**\n",
    "\n",
    "Let's create a synthetic time series. We will overlap 2 frequencies with noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size,n_steps):\n",
    "    f1,f2,off1,off2=np.random.rand(4,batch_size,1)\n",
    "    t = np.linspace(0,1,n_steps)\n",
    "    y = 0.5*np.sin( (t-off1)*(f1*10+10) ) # first wave\n",
    "    y += 0.5*np.sin( (t-off2)*(f2*20+20) ) # second wave\n",
    "    y += 0.3* (np.random.rand(batch_size,n_steps)-0.5) # noise\n",
    "    return y.astype(np.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate 10k time series of context+prediction_horizon points.\n",
    "context=50\n",
    "prediction_horizon=10\n",
    "data = generate_time_series(10000,context+prediction_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some of the time series in a 2x3 grid\n",
    "plt.figure(figsize=(6,4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    # plot context data\n",
    "    plt.plot(range(context),data[i,:context],'b-',label='context')\n",
    "    # plot prediction horizon\n",
    "    plt.plot(range(context,context+prediction_horizon),data[i,context:],'r-',label='prediction')\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design a vanilla RNN\n",
    "\n",
    "The RNN will take context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN layer, batch_size is the first dimension\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size) # Fully connected layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x) # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc(out[:, -1, :]) # out: tensor of shape (batch_size, output_size)\n",
    "        return out\n",
    "\n",
    "def create_rnn_and_data(time_series, context, prediction_horizon):\n",
    "    input_size = time_series.shape[-1]\n",
    "    hidden_size = 20  # You can adjust this value\n",
    "    output_size = prediction_horizon\n",
    "    \n",
    "    model = VanillaRNN(input_size, hidden_size, output_size)\n",
    "    \n",
    "    # Prepare the input and target data\n",
    "    x = time_series[:, :context,np.newaxis ] # Add a dummy dimension for the input size\n",
    "    y = time_series[:, context:context + prediction_horizon]\n",
    "    \n",
    "    return model, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, x, y = create_rnn_and_data(data, context, prediction_horizon)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now prepare the training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, x_val, y_val, n_epochs=100, lr=0.001):\n",
    "\n",
    "    '''\n",
    "    Function to train the model.\n",
    "\n",
    "    Parameters:\n",
    "    model: torch.nn.Module\n",
    "        The model to train\n",
    "    x_train: np.ndarray\n",
    "        The input data for training (context)\n",
    "    y_train: np.ndarray\n",
    "        The target data for training (prediction horizon)\n",
    "    x_val: np.ndarray\n",
    "        The input data for validation (context)\n",
    "    y_val: np.ndarray\n",
    "        The target data for validation (prediction horizon)\n",
    "    n_epochs: int\n",
    "        The number of epochs\n",
    "    lr: float\n",
    "        The learning rate\n",
    "\n",
    "    Returns:\n",
    "    train_losses: list\n",
    "        The training losses for each epoch\n",
    "    val_losses: list\n",
    "        The validation losses for each epoch    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(torch.from_numpy(x_train).float())\n",
    "        print(f\"Shape of outputs: {y_pred.shape}\")\n",
    "        print(f\"Shape of y_train: {y_train.shape}\")\n",
    "\n",
    "        loss = loss_fn(y_pred, torch.from_numpy(y_train).float())\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_val_pred = model(torch.from_numpy(x_val).float())\n",
    "                print(f\"Shape of val_outputs: {y_val_pred.shape}\")\n",
    "                print(f\"Shape of y_val: {y_val.shape}\")\n",
    "  \n",
    "                val_loss = loss_fn(y_val_pred, torch.from_numpy(y_val).float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            print(f'Epoch {epoch}, Loss {loss.item():.6f}, Val loss {val_loss.item():.6f}')\n",
    "    \n",
    "    return train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the VanillaRNN by first splitting the data between training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_horizon, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split y into training and validation sets\n",
    "n_train = 7000\n",
    "x_train, y_train = data[:n_train, :context], data[:n_train, context:context + prediction_horizon]\n",
    "x_val, y_val = data[n_train:, :context], data[n_train:, context:context + prediction_horizon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the last dimnsion of y_train and y_val\n",
    "y_train = y_train.squeeze()\n",
    "y_val = y_val.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses = train(model, x_train, y_train, x_val, y_val, n_epochs=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curves\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(val_losses, label='val')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now show examples on the validation set with the ground truth and the prediction\n",
    "# add a dimension to x_val\n",
    "# x_val = x_val[...,np.newaxis]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_val_pred = model(torch.from_numpy(x_val).float()).numpy()\n",
    "\n",
    "# plot some of the time series in a 2x3 grid\n",
    "plt.figure(figsize=(6,4))\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.plot(np.concatenate([x_val[i],y_val[i]]), label='GT')\n",
    "    plt.plot(np.concatenate([x_val[i],y_val_pred[i,np.newaxis].T]), label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast of several steps ahead: how far can you predict the future?\n",
    "We will try and predict 10 steps ahead. The early part of the forecast will be a lot better than the later part of the forecast as uncertainties increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate 10k time series of 51 points.\n",
    "n_steps=50\n",
    "x = generate_time_series(10000,n_steps+10)\n",
    "y=np.empty((10000,n_steps,10))\n",
    "for step_ahead in range(1,10+1):\n",
    "    y[:,:,step_ahead-1]=x[:,step_ahead:step_ahead+n_steps,0]\n",
    "\n",
    "    \n",
    "x_train=x[:7000,:n_steps]\n",
    "x_val=x[7000:9000,:n_steps]\n",
    "x_test=x[9000:,:n_steps]\n",
    "\n",
    "y_train=y[:7000]\n",
    "y_val=y[7000:9000]\n",
    "y_test=y[9000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20,input_shape=[None,1],return_sequences=True),\n",
    "    keras.layers.SimpleRNN(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "history=model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=20, batch_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "print(y_pred.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(n_steps+10),x[9000,:])\n",
    "plt.plot(np.arange(n_steps),x_test[0,:])\n",
    "plt.plot(np.arange(10)+n_steps,y_pred[0,-1,:],'+')\n",
    "plt.legend(('Truth','past','future'))\n",
    "plt.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with RNNs and solutions\n",
    "\n",
    "Simple RNNs have issues during the training with backpropagation that gradients may become too small and that the model no longer updates during training. This is called the **vanishing gradient** problem.\n",
    "\n",
    "To remedy this, the algorithm \"LSTM\" introduces a memory-cell and gating to allow and reset the values and avoid vanishing gradients.\n",
    "\n",
    "\n",
    "### 2. LSTM\n",
    "Long-Short Term Memory are (somewhat complicated) cells that aims to solve the memory loss issue.\n",
    "\n",
    "![LSTM](../img/lstm-2.svg)\n",
    "An LSTM combines hidden state from the previous layers, the memory of the internal state, and the input data to output the current hidden and internal states.\n",
    "<!-- <img src=\"./figures/lstm.png\" alt=\"lstm\" style=\"width: 400px;\"/> -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.models.Sequential([\n",
    "    keras.layers.LSTM(20,input_shape=[None,1],return_sequences=True),\n",
    "    keras.layers.LSTM(20,return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse',metrics=['mse'])\n",
    "history=model.fit(x_train,y_train,validation_data=(x_val,y_val), epochs=20, batch_size=128) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "plt.plot(np.arange(n_steps+10),x[9000,:])\n",
    "plt.plot(np.arange(n_steps),x_test[0,:])\n",
    "plt.plot(np.arange(10)+n_steps,y_pred[0,-1,:],'+')\n",
    "plt.legend(('Truth','past','future'))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
